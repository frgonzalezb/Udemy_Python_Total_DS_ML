{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines y Automatización de Flujo de Trabajo en Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un _pipeline_ es una herramienta de automatización de procesos de aprendizaje automático, tanto para la implementación como la evaluación de los modelos.\n",
    "\n",
    "Para entenderlo mejor, el concepto proviene naturalmente de las tuberías. Un _pipeline_ es algo que **canaliza** un elemento fluido para que se transporte desde un lugar a otro. Por ende, se utiliza como una metáfora para describir procesos que tienen una **secuencia lineal y continua**, donde **una fase lleva a la siguiente**. Es decir, al terminar una fase, comienza otra directamente. Por ejemplo, en la industria panadera, un _pipeline_ podría referirse al flujo de producción que describe todo el recorrido que hace la materia prima (el trigo) hasta el producto final (el pan).\n",
    "\n",
    "En Scikit-Learn, un _pipeline_ es una **secuencia de transformaciones** que terminan en un **modelo final** de machine learning. Encapsula todas las secuencias de pasos (por ejemplo, las que se han visto hasta ahora en el curso), asegurándonos que todos ellos se ejecuten en el orden correcto, de una forma repetible, fiable y consistente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos el código usado en la [clase introductoria](intro.ipynb) (sin pipelines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "El puntaje para la precisión del modelo es: 1.00\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "scaler: StandardScaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model: LogisticRegression = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred: np.ndarray = model.predict(X_test_scaled)\n",
    "\n",
    "score: float = float(model.score(X_test_scaled, y_test))\n",
    "\n",
    "print(f'Las predicciones son: {y_pred}')\n",
    "print(f'El puntaje para la precisión del modelo es: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, copiemos el mismo código y apliquemos aquí el concepto de pipelines, desde `from sklearn.pipeline import Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "El puntaje para la precisión del modelo es: 1.00\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# por aquí va el pipeline!\n",
    "pipeline: Pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression(random_state=42, max_iter=1000))\n",
    "])\n",
    "\n",
    "\"\"\"\n",
    "CÓDIGO LIBERADO POR EL PIPELINE:\n",
    "\n",
    "scaler: StandardScaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model: LogisticRegression = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\"\"\"\n",
    "\n",
    "# entrenar al pipeline!\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predicciones con el pipeline\n",
    "y_pred: np.ndarray | tuple = pipeline.predict(X_test)\n",
    "score: float = float(pipeline.score(X_test, y_test))\n",
    "\n",
    "print(f'Las predicciones son: {y_pred}')\n",
    "print(f'El puntaje para la precisión del modelo es: {score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **ventaja** de tener a `scaler` y `model` en la tubería es que ya no es necesario entrenar dichos elementos por separado: **solo basta con entrenar al pipeline directamente** (ya que aplicará esto para sus elementos internos). Adicionalmente, la necesidad de transformar explícitamente los datos se elimina, ya que el pipeline gestiona las transformaciones por su cuenta internamente y se asegura de que se apliquen de manera adecuada, en el orden correcto durante la fase de entrenamiento y la fase de predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionalmente, todo esto se puede simplificar aún más con `make_pipelines`, donde ya no es necesario declarar explícitamente el nombre de los elementos (`scaler` y `model` en el ejemplo). Usemos el código recién creado como base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Las predicciones son: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1\n",
      " 0 0 0 2 1 1 0 0]\n",
      "El puntaje para la precisión del modelo es: 1.00\n"
     ]
    }
   ],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# por aquí va el pipeline!\n",
    "pipeline: Pipeline = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(random_state=42, max_iter=1000)\n",
    ")\n",
    "\n",
    "# entrenar al pipeline!\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predicciones con el pipeline\n",
    "y_pred: np.ndarray | tuple = pipeline.predict(X_test)\n",
    "score: float = float(pipeline.score(X_test, y_test))\n",
    "\n",
    "print(f'Las predicciones son: {y_pred}')\n",
    "print(f'El puntaje para la precisión del modelo es: {score:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
