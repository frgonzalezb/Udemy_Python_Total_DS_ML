{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7bf2362",
   "metadata": {},
   "source": [
    "## Proyecto del Día 13 - Juego de Navegación en un Laberinto\n",
    "\n",
    "Este proyecto tiene como objetivo desarrollar un entorno de laberinto simple y aplicar un algoritmo de **Aprendizaje por Refuerzo** para enseñar a una IA a navegar desde un punto inicial hasta un objetivo.\n",
    "\n",
    "Dada la naturaleza de este proyecto, considero que el algoritmo más adecuado para este tipo de probleas es **Q-Learning**, por su facilidad de implelentación y comprensión, su estabilidad y su relación entre la exploración y la explotación.\n",
    "\n",
    "Por esa razón te propongo resolverlo usando ese algoritmo, aunque dejo a tu criterio si quieres resolverlo con otro algoritmo de tu elección. Siempre estaré a favor de que investigues, y expandas las habilidades propuestas por tu cuenta.\n",
    "\n",
    "### Descripción del Laberinto:\n",
    "\n",
    "El laberinto se representa como una matriz de dos dimensiones, donde cada elemento puede ser:\n",
    "+ un camino libre (0)\n",
    "+ un obstáculo (1)\n",
    "+ el objetivo (G)\n",
    "\n",
    "La tarea es desarrollar un agente que pueda aprender a encontrar el camino desde un punto de inicio hasta el objetivo evitando obstáculos.\n",
    "\n",
    "\n",
    "### Creación del Laberinto\n",
    "\n",
    "Debido a que el desafío de hoy es bastante complejo, y que el objetivo final no se trata de que sepas desarrollar laberintos, sino sistemas para resolverlos, voy a facilitar la tarea entregando en este cuaderno el código para generar nuestros laberintos.\n",
    "\n",
    "Tu parte será la siguiente, que es diseñar y entrenar un modelo de Q-Learning para resolver el laberinto de la manera mpas eficiente, y luego mostrar una visualización sobre cómo lo ha hecho.\n",
    "\n",
    "Te deseo toda la suerte del mundo, y sobre todo, que te diviertas de a montones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e7ecc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías necesarias para todo el ejercicio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6370346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el laberinto\n",
    "def crear_laberinto(tamanio, porcentaje_paredes=20, inicio=(0, 0), meta=None):\n",
    "    laberinto = np.zeros((tamanio, tamanio))\n",
    "    numero_paredes = int((tamanio * tamanio) * porcentaje_paredes / 100)\n",
    "    \n",
    "    # EDIT: Definir el estado de los obstáculos\n",
    "    obstaculos: list[tuple[int, int]] = []\n",
    "\n",
    "    # Ubicar paredes\n",
    "    for pared in range(numero_paredes):\n",
    "        x, y = random.randint(0, tamanio-1), random.randint(0, tamanio-1)\n",
    "        \n",
    "        # Cuidar que inicio y meta no sean paredes\n",
    "        if (x, y) != inicio and (meta is None or (x, y) != meta):\n",
    "            laberinto[x, y] = 1\n",
    "            obstaculos.append((x, y))\n",
    "            \n",
    "    # Ubicar la meta\n",
    "    if meta:\n",
    "        laberinto[meta] = 9  # Representa la meta con 9\n",
    "    else:\n",
    "        # Ubicar la meta aleatoriamente si no está especificado\n",
    "        while True:\n",
    "            x, y = random.randint(0, tamanio-1), random.randint(0, tamanio-1)\n",
    "            if laberinto[x, y] == 0 and (x, y) != inicio:\n",
    "                laberinto[x, y] = 9\n",
    "                break\n",
    "    \n",
    "    return laberinto, obstaculos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c648cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para visualizar el laberinto\n",
    "def ver_laberinto(laberinto):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(laberinto, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10e052cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ejemplo de crear y mostrar laberintos\n",
    "laberinto, obstaculos = crear_laberinto(10, 20, inicio=(0, 0), meta=(9, 9))\n",
    "ver_laberinto(laberinto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75336c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 1., 0., 1., 1., 1., 0., 0.],\n",
       "       [1., 0., 1., 1., 0., 0., 1., 0., 0., 9.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laberinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ef15cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laberinto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bb319ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 5),\n",
       " (7, 9),\n",
       " (4, 3),\n",
       " (9, 6),\n",
       " (8, 2),\n",
       " (8, 6),\n",
       " (9, 3),\n",
       " (8, 7),\n",
       " (5, 1),\n",
       " (3, 8),\n",
       " (0, 1),\n",
       " (1, 7),\n",
       " (2, 7),\n",
       " (9, 0),\n",
       " (7, 1),\n",
       " (8, 5),\n",
       " (1, 1),\n",
       " (9, 2),\n",
       " (8, 3),\n",
       " (4, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obstaculos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f986ed5",
   "metadata": {},
   "source": [
    "### Ahora te toca a ti\n",
    "\n",
    "Lo que sigue es implementar todo el código para que un algoritmo de Q-Learning encuentre la manera más eficiente de llegar a la meta. Voy a dejarte los pasos que considero que son los necesarios para lograrlo\n",
    "\n",
    "##### 1. Parámetros para el algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "234853df",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state: tuple[int, int] = (0, 0)\n",
    "goal_state: tuple[int, int] = (9, 9)\n",
    "\n",
    "directions: list[str] = ['up', 'down', 'left', 'right']\n",
    "\n",
    "actions: dict[int, tuple[int, int]] = {\n",
    "    directions.index('up'): (-1, 0),\n",
    "    directions.index('down'): (1, 0),\n",
    "    directions.index('left'): (0, -1),\n",
    "    directions.index('right'): (0, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1291a4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_states: int = laberinto.shape[0] * laberinto.shape[1]\n",
    "possible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e36682e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions: int = len(actions)\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f4da219",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha: float = 0.1      # learning rate\n",
    "gamma: float = 0.99     # discount factor\n",
    "epsilon: float = 0.2    # exploration\n",
    "episodes: int = 100     # number of episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9397dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125709aa",
   "metadata": {},
   "source": [
    "##### 2. Función para elegir acciones equilibrando entre explotación y exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abbd3e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(\n",
    "        Q: np.ndarray,\n",
    "        state: tuple[int, int],\n",
    "        possible_actions: int,\n",
    "        grid: tuple[int, int],\n",
    "        epsilon: float\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Choose a random action or the best action for the current state,\n",
    "    depending on the exploration rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "    \n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "    \n",
    "    possible_actions : int\n",
    "        The quantity of possible actions for the agent.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    epsilon : float\n",
    "        The exploration rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        A random action or the best action for the current state,\n",
    "        depending on the exploration rate.\n",
    "    \n",
    "    Requirements\n",
    "    ------------\n",
    "    1. Python's built-in `random` library (`import random`).\n",
    "    2. NumPy (`import numpy as np`).\n",
    "    3. The `convert_state_to_index()` function declared above.        \n",
    "    \"\"\"\n",
    "    random_number: float = random.uniform(0, 1)\n",
    "    random_action: int = random.choice(range(possible_actions))\n",
    "    best_action: int = int(np.argmax(Q[convert_state_to_index(state, grid)]))\n",
    "\n",
    "    if random_number < epsilon:\n",
    "        return random_action\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003a6550",
   "metadata": {},
   "source": [
    "##### 3. Función para simular la acción en el laberinto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f719b2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(\n",
    "        action: tuple[int, int],\n",
    "        state: tuple[int, int],\n",
    "        goal_state: tuple[int, int],\n",
    "        grid: tuple[int, int],\n",
    "        obstacles: list[tuple[int, int]]\n",
    "        ) -> tuple[tuple[int, int], int, bool]:\n",
    "    \"\"\"\n",
    "    Apply the action to the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    action : tuple[int, int]\n",
    "        The action to be applied.\n",
    "\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    goal_state : tuple[int, int]\n",
    "        The two-dimensional representation of the state declared as\n",
    "        goal.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    obstacles : list[tuple[int, int]]\n",
    "        The list of obstacles on the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple[int, int], int, bool]\n",
    "        The new state, the reward, and whether the game is over.\n",
    "\n",
    "    \"\"\"\n",
    "    points: int = 0\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    new_state: tuple[int, int] = tuple(np.add(state, action) % grid)\n",
    "\n",
    "    if new_state in obstacles or new_state == state:\n",
    "        points = -100\n",
    "        return state, points, is_game_over\n",
    "    elif new_state == goal_state:\n",
    "        points = 100\n",
    "        is_game_over = True\n",
    "        return new_state, points, is_game_over\n",
    "    else:\n",
    "        points = -1\n",
    "        return new_state, points, is_game_over\n",
    "    # points: int = 0\n",
    "    # is_game_over: bool = False\n",
    "    # new_state: tuple[int, int] = (state[0] + action[0], state[1] + action[1])\n",
    "\n",
    "    # if (0 <= new_state[0] < grid[0]) and (0 <= new_state[1] < grid[1]):\n",
    "    #     if new_state in obstacles:\n",
    "    #         points = -100\n",
    "    #         return state, points, is_game_over\n",
    "    #     elif new_state == goal_state:\n",
    "    #         points = 100\n",
    "    #         is_game_over = True\n",
    "    #         return new_state, points, is_game_over\n",
    "    #     else:\n",
    "    #         points = -1\n",
    "    #         return new_state, points, is_game_over\n",
    "    # else:\n",
    "    #     points = -1\n",
    "    #     return new_state, points, is_game_over"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3dc48",
   "metadata": {},
   "source": [
    "##### 4. Función principal para ejecutar el algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ee5259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_values(\n",
    "        Q: np.ndarray,\n",
    "        state_index: int,\n",
    "        action_index: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        new_state_index: int,\n",
    "        reward: int,\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    Update the Q values for the current state and action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "\n",
    "    state_index : tuple[int, int]\n",
    "        The index of the current state in the Q table.\n",
    "\n",
    "    action_index : int\n",
    "        The index of the current action in the Q table.\n",
    "\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "\n",
    "    gamma : float\n",
    "        The discount factor.\n",
    "\n",
    "    new_state_index : tuple[int, int]\n",
    "        The index of the new state in the Q table.\n",
    "\n",
    "    reward : int\n",
    "        The reward for the current state and action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The updated Q value.\n",
    "    \"\"\"\n",
    "    current_q: np.ndarray = Q[state_index, action_index]\n",
    "    best_future_q: int = int(np.max(Q[new_state_index]))\n",
    "    updated_q: float = float(\n",
    "        current_q + alpha * (reward + (gamma * best_future_q) - current_q)\n",
    "    )\n",
    "    return updated_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac97336",
   "metadata": {},
   "source": [
    "##### 5. Función para convertir coordenadas a índice lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5311d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f934cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, n = laberinto.shape\n",
    "state_ex = (1, 0)\n",
    "grid_ex = (m, n)\n",
    "example = convert_state_to_index(state_ex, grid_ex)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f0d38",
   "metadata": {},
   "source": [
    "##### 6. Iniciar el laberinto y configurar el algoritmo Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3f5bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as log  # para mantener registro de cada paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2355b8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = log.getLogger(__name__)\n",
    "logger.setLevel(log.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    console_handler: log.StreamHandler = log.StreamHandler()\n",
    "    console_handler.setLevel(log.INFO)\n",
    "\n",
    "    formatter: log.Formatter = log.Formatter(\n",
    "        '%(message)s'\n",
    "    )\n",
    "\n",
    "    logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0edcc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 10.0\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 0, Score: 100\n",
      "\n",
      "\n",
      "Episode: 1\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -0.19\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (2, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 20\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 0)\n",
      "\n",
      "\tState index: 20\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 0.8900000000000001\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 19.0\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 1, Score: 100\n",
      "\n",
      "\n",
      "Episode: 2\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 1.691\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 27.1\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 2, Score: 100\n",
      "\n",
      "\n",
      "Episode: 3\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 4.0949\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 34.39\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 3, Score: 100\n",
      "\n",
      "\n",
      "Episode: 4\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 6.95141\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 40.951\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 4, Score: 100\n",
      "\n",
      "\n",
      "Episode: 5\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 10.116269\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 98\n",
      "\tQ value: -0.1\n",
      "\tState: (9, 8)\n",
      "\n",
      "\tState index: 98\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 10.0\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 5, Score: 100\n",
      "\n",
      "\n",
      "Episode: 6\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 12.9646421\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 46.8559\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 6, Score: 100\n",
      "\n",
      "\n",
      "Episode: 7\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 16.12217789\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 52.17031\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 7, Score: 100\n",
      "\n",
      "\n",
      "Episode: 8\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 19.557960101\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 56.953279\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 8, Score: 100\n",
      "\n",
      "\n",
      "Episode: 9\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 23.0461640909\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 61.2579511\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 9, Score: 100\n",
      "\n",
      "\n",
      "Episode: 10\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -0.271\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -10.0\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 2.087\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 26.68054768181\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 65.13215599\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 10, Score: 100\n",
      "\n",
      "\n",
      "Episode: 11\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 30.347492913629\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 68.618940391\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 11, Score: 100\n",
      "\n",
      "\n",
      "Episode: 12\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 33.9447436222661\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 71.7570463519\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 12, Score: 100\n",
      "\n",
      "\n",
      "Episode: 13\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 37.479269260039494\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 7.73\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 3.5630000000000006\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -15.337\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 40.66034233403555\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 74.58134171671\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 13, Score: 100\n",
      "\n",
      "\n",
      "Episode: 14\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 43.82030810063199\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 77.123207545039\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 14, Score: 100\n",
      "\n",
      "\n",
      "Episode: 15\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 46.961277290568795\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 79.4108867905351\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 15, Score: 100\n",
      "\n",
      "\n",
      "Episode: 16\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 49.986149561511915\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: -0.19\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 8)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 7)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 7\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 7)\n",
      "\n",
      "\tState index: 7\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 7)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 97\n",
      "\tQ value: -0.1\n",
      "\tState: (9, 7)\n",
      "\n",
      "\tState index: 97\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 7)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 97\n",
      "\tQ value: -10.0\n",
      "\tState: (9, 7)\n",
      "\n",
      "\tState index: 97\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (0, 7)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 7\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 7)\n",
      "\n",
      "\tState index: 7\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (0, 7)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 7\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 7)\n",
      "\n",
      "\tState index: 7\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 6\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 6)\n",
      "\n",
      "\tState index: 6\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 16\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 6)\n",
      "\n",
      "\tState index: 16\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 6\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 6)\n",
      "\n",
      "\tState index: 6\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 5)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 5\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 5)\n",
      "\n",
      "\tState index: 5\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 5)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 95\n",
      "\tQ value: -0.1\n",
      "\tState: (9, 5)\n",
      "\n",
      "\tState index: 95\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 5)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 95\n",
      "\tQ value: -10.0\n",
      "\tState: (9, 5)\n",
      "\n",
      "\tState index: 95\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (0, 5)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 5\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 5)\n",
      "\n",
      "\tState index: 5\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (0, 5)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 5\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 5)\n",
      "\n",
      "\tState index: 5\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 4\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 4)\n",
      "\n",
      "\tState index: 4\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 94\n",
      "\tQ value: -0.1\n",
      "\tState: (9, 4)\n",
      "\n",
      "\tState index: 94\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (8, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 84\n",
      "\tQ value: -0.1\n",
      "\tState: (8, 4)\n",
      "\n",
      "\tState index: 84\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (7, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 74\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 4)\n",
      "\n",
      "\tState index: 74\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (6, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 64\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 4)\n",
      "\n",
      "\tState index: 64\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (7, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 74\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 4)\n",
      "\n",
      "\tState index: 74\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (7, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 73\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 3)\n",
      "\n",
      "\tState index: 73\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (6, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 63\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 3)\n",
      "\n",
      "\tState index: 63\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (5, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 53\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 3)\n",
      "\n",
      "\tState index: 53\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (5, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 54\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 4)\n",
      "\n",
      "\tState index: 54\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (4, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 44\n",
      "\tQ value: -0.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tState index: 44\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (3, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 34\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 4)\n",
      "\n",
      "\tState index: 34\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (2, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 24\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 4)\n",
      "\n",
      "\tState index: 24\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (1, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 14\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 4)\n",
      "\n",
      "\tState index: 14\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 4\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 4)\n",
      "\n",
      "\tState index: 4\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 14\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 4)\n",
      "\n",
      "\tState index: 14\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (2, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 24\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 4)\n",
      "\n",
      "\tState index: 24\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (3, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 34\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 4)\n",
      "\n",
      "\tState index: 34\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (4, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 44\n",
      "\tQ value: -0.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tState index: 44\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (5, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 54\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 4)\n",
      "\n",
      "\tState index: 54\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (6, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 64\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 4)\n",
      "\n",
      "\tState index: 64\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (5, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 54\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 4)\n",
      "\n",
      "\tState index: 54\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (5, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 53\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 3)\n",
      "\n",
      "\tState index: 53\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (5, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 53\n",
      "\tQ value: -10.0\n",
      "\tState: (5, 3)\n",
      "\n",
      "\tState index: 53\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (6, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 63\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 3)\n",
      "\n",
      "\tState index: 63\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (7, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 73\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 3)\n",
      "\n",
      "\tState index: 73\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (7, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 73\n",
      "\tQ value: -10.0\n",
      "\tState: (7, 3)\n",
      "\n",
      "\tState index: 73\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (7, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 72\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 2)\n",
      "\n",
      "\tState index: 72\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (6, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 62\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 2)\n",
      "\n",
      "\tState index: 62\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (5, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 52\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 2)\n",
      "\n",
      "\tState index: 52\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (4, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 42\n",
      "\tQ value: -0.1\n",
      "\tState: (4, 2)\n",
      "\n",
      "\tState index: 42\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (3, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 32\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 2)\n",
      "\n",
      "\tState index: 32\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 22\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 22\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (1, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 2\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 2)\n",
      "\n",
      "\tState index: 2\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 2\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 2)\n",
      "\n",
      "\tState index: 2\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 22\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 22\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (3, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 32\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 2)\n",
      "\n",
      "\tState index: 32\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (4, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 42\n",
      "\tQ value: -0.1\n",
      "\tState: (4, 2)\n",
      "\n",
      "\tState index: 42\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (5, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 52\n",
      "\tQ value: -0.1\n",
      "\tState: (5, 2)\n",
      "\n",
      "\tState index: 52\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (6, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 62\n",
      "\tQ value: -0.1\n",
      "\tState: (6, 2)\n",
      "\n",
      "\tState index: 62\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (7, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 72\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 2)\n",
      "\n",
      "\tState index: 72\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (7, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 72\n",
      "\tQ value: -10.0\n",
      "\tState: (7, 2)\n",
      "\n",
      "\tState index: 72\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (7, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 72\n",
      "\tQ value: -10.0\n",
      "\tState: (7, 2)\n",
      "\n",
      "\tState index: 72\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (7, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 73\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 3)\n",
      "\n",
      "\tState index: 73\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (7, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 74\n",
      "\tQ value: -0.1\n",
      "\tState: (7, 4)\n",
      "\n",
      "\tState index: 74\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (8, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 84\n",
      "\tQ value: -0.1\n",
      "\tState: (8, 4)\n",
      "\n",
      "\tState index: 84\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (9, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 94\n",
      "\tQ value: -0.1\n",
      "\tState: (9, 4)\n",
      "\n",
      "\tState index: 94\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (0, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 4\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 4)\n",
      "\n",
      "\tState index: 4\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 3\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 3)\n",
      "\n",
      "\tState index: 3\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 3\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 3)\n",
      "\n",
      "\tState index: 3\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 3\n",
      "\tQ value: -19.0\n",
      "\tState: (0, 3)\n",
      "\n",
      "\tState index: 3\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 4)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 4\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 4)\n",
      "\n",
      "\tState index: 4\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 5)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 5\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 5)\n",
      "\n",
      "\tState index: 5\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 6\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 6)\n",
      "\n",
      "\tState index: 6\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 16\n",
      "\tQ value: -0.19\n",
      "\tState: (1, 6)\n",
      "\n",
      "\tState index: 16\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (2, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 26\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 6)\n",
      "\n",
      "\tState index: 26\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (1, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 16\n",
      "\tQ value: -0.1\n",
      "\tState: (1, 6)\n",
      "\n",
      "\tState index: 16\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (1, 6)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 16\n",
      "\tQ value: -10.0\n",
      "\tState: (1, 6)\n",
      "\n",
      "\tState index: 16\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 6)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 6\n",
      "\tQ value: -0.19\n",
      "\tState: (0, 6)\n",
      "\n",
      "\tState index: 6\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 6)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 6\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 6)\n",
      "\n",
      "\tState index: 6\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 7)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 7\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 7)\n",
      "\n",
      "\tState index: 7\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: -0.1\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 7.721\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 81.4697981114816\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 16, Score: 100\n",
      "\n",
      "\n",
      "Episode: 17\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 52.906534605360726\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 83.32281830033344\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 17, Score: 100\n",
      "\n",
      "\n",
      "Episode: 18\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 55.73288114482465\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 84.9905364703001\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 18, Score: 100\n",
      "\n",
      "\n",
      "Episode: 19\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 58.37559303034219\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 86.49148282327009\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 19, Score: 100\n",
      "\n",
      "\n",
      "Episode: 20\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 60.95203372730797\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 87.84233454094309\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 20, Score: 100\n",
      "\n",
      "\n",
      "Episode: 21\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 63.369830354577175\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: 0.503\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 15.469999999999999\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 89.05810108684878\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 21, Score: 100\n",
      "\n",
      "\n",
      "Episode: 22\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 65.74384731911945\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 90.1522909781639\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 22, Score: 100\n",
      "\n",
      "\n",
      "Episode: 23\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 67.97946258720751\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 91.13706188034752\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 23, Score: 100\n",
      "\n",
      "\n",
      "Episode: 24\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 70.09051632848676\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 92.02335569231276\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 24, Score: 100\n",
      "\n",
      "\n",
      "Episode: 25\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -16.8733\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 72.08946469563809\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 92.82102012308148\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 25, Score: 100\n",
      "\n",
      "\n",
      "Episode: 26\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 73.88851822607428\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 93.53891811077334\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 26, Score: 100\n",
      "\n",
      "\n",
      "Episode: 27\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 75.60666640346685\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 94.185026299696\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 27, Score: 100\n",
      "\n",
      "\n",
      "Episode: 28\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 77.25199976312017\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 94.76652366972641\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 28, Score: 100\n",
      "\n",
      "\n",
      "Episode: 29\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 78.73279978680816\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 0.4220000000000001\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 16.154899999999998\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 95.28987130275377\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 29, Score: 100\n",
      "\n",
      "\n",
      "Episode: 30\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 80.16451980812734\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 95.76088417247838\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 30, Score: 100\n",
      "\n",
      "\n",
      "Episode: 31\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 81.4530678273146\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 96.18479575523054\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 31, Score: 100\n",
      "\n",
      "\n",
      "Episode: 32\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 82.71176104458314\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 96.56631617970748\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 32, Score: 100\n",
      "\n",
      "\n",
      "Episode: 33\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 83.84458494012483\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 96.90968456173674\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 33, Score: 100\n",
      "\n",
      "\n",
      "Episode: 34\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 84.86412644611235\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 97.21871610556306\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 34, Score: 100\n",
      "\n",
      "\n",
      "Episode: 35\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 85.88071380150112\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 97.49684449500675\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 35, Score: 100\n",
      "\n",
      "\n",
      "Episode: 36\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 86.795642421351\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 97.74716004550608\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 36, Score: 100\n",
      "\n",
      "\n",
      "Episode: 37\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 87.6190781792159\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 97.97244404095547\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 37, Score: 100\n",
      "\n",
      "\n",
      "Episode: 38\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 88.36017036129431\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.17519963685992\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 38, Score: 100\n",
      "\n",
      "\n",
      "Episode: 39\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 89.12615332516488\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.35767967317392\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 39, Score: 100\n",
      "\n",
      "\n",
      "Episode: 40\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 89.81553799264839\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.52191170585652\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 40, Score: 100\n",
      "\n",
      "\n",
      "Episode: 41\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 90.43598419338355\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.66972053527087\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 41, Score: 100\n",
      "\n",
      "\n",
      "Episode: 42\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 90.99438577404518\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.80274848174378\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 42, Score: 100\n",
      "\n",
      "\n",
      "Episode: 43\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 91.49694719664066\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 98.9224736335694\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 43, Score: 100\n",
      "\n",
      "\n",
      "Episode: 44\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -16.17697\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 91.9492524769766\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.03022627021245\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 44, Score: 100\n",
      "\n",
      "\n",
      "Episode: 45\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 92.45532722927894\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.12720364319121\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 45, Score: 100\n",
      "\n",
      "\n",
      "Episode: 46\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 92.91079450635104\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.21448327887208\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 46, Score: 100\n",
      "\n",
      "\n",
      "Episode: 47\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 93.32071505571594\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.29303495098488\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 47, Score: 100\n",
      "\n",
      "\n",
      "Episode: 48\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 93.68964355014434\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 1.8638000000000001\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 24.240409999999997\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.36373145588638\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 48, Score: 100\n",
      "\n",
      "\n",
      "Episode: 49\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 94.0216791951299\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: 1.8377\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 23.624000000000002\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 3.9534199999999995\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 31.517369\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.42735831029775\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 49, Score: 100\n",
      "\n",
      "\n",
      "Episode: 50\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 94.32051127561692\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.48462247926797\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 50, Score: 100\n",
      "\n",
      "\n",
      "Episode: 51\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 94.58946014805522\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.53616023134117\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 51, Score: 100\n",
      "\n",
      "\n",
      "Episode: 52\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 94.8315141332497\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.58254420820705\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 52, Score: 100\n",
      "\n",
      "\n",
      "Episode: 53\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.04936271992473\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.62428978738635\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 53, Score: 100\n",
      "\n",
      "\n",
      "Episode: 54\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.24542644793226\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.66186080864772\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 54, Score: 100\n",
      "\n",
      "\n",
      "Episode: 55\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.42188380313904\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.69567472778294\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 55, Score: 100\n",
      "\n",
      "\n",
      "Episode: 56\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.58069542282513\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.72610725500465\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 56, Score: 100\n",
      "\n",
      "\n",
      "Episode: 57\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.72362588054261\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 6.5270779999999995\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 38.0666321\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.75349652950419\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 57, Score: 100\n",
      "\n",
      "\n",
      "Episode: 58\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -9.595\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.85226329248835\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.77814687655378\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 58, Score: 100\n",
      "\n",
      "\n",
      "Episode: 59\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 95.96803696323951\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.8003321888984\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 59, Score: 100\n",
      "\n",
      "\n",
      "Episode: 60\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.07223326691556\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.82029897000855\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 60, Score: 100\n",
      "\n",
      "\n",
      "Episode: 61\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: -0.14590000000000003\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 11.2823\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -9.1315\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.166009940224\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 9.5363702\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 98\n",
      "\tQ value: 0.8\n",
      "\tState: (9, 8)\n",
      "\n",
      "\tState index: 98\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 19.0\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 61, Score: 100\n",
      "\n",
      "\n",
      "Episode: 62\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.2504089462016\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.8382690730077\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 62, Score: 100\n",
      "\n",
      "\n",
      "Episode: 63\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.32636805158144\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.85444216570693\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 63, Score: 100\n",
      "\n",
      "\n",
      "Episode: 64\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.39473124642329\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 12.24473318\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 43.960968890000004\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.86899794913623\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 64, Score: 100\n",
      "\n",
      "\n",
      "Episode: 65\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.45625812178096\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: 3.8309299999999995\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 30.962600000000002\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.8820981542226\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 65, Score: 100\n",
      "\n",
      "\n",
      "Episode: 66\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.51163230960287\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 15.177259862000001\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 98\n",
      "\tQ value: 2.501\n",
      "\tState: (9, 8)\n",
      "\n",
      "\tState index: 98\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 27.1\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 66, Score: 100\n",
      "\n",
      "\n",
      "Episode: 67\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.56146907864259\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.89388833880034\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 67, Score: 100\n",
      "\n",
      "\n",
      "Episode: 68\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.60632217077833\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 17.8165338758\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 49.265872001000005\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.90449950492031\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 68, Score: 100\n",
      "\n",
      "\n",
      "Episode: 69\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.6466899537005\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.91404955442827\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 69, Score: 100\n",
      "\n",
      "\n",
      "Episode: 70\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.68302095833045\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.92264459898544\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 70, Score: 100\n",
      "\n",
      "\n",
      "Episode: 71\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.7157188624974\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.93038013908689\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 71, Score: 100\n",
      "\n",
      "\n",
      "Episode: 72\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: 0.85769\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 19.55807\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.74514697624765\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.9373421251782\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 72, Score: 100\n",
      "\n",
      "\n",
      "Episode: 73\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.77163227862289\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.94360791266038\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 73, Score: 100\n",
      "\n",
      "\n",
      "Episode: 74\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.7954690507606\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.94924712139435\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 74, Score: 100\n",
      "\n",
      "\n",
      "Episode: 75\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.81692214568454\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 8)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 8\n",
      "\tQ value: 20.785880488220002\n",
      "\tState: (0, 8)\n",
      "\n",
      "\tState index: 8\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 54.040284800900004\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: 6.317837\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (2, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 29\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 9)\n",
      "\n",
      "\tState index: 29\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (1, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 19\n",
      "\tQ value: 2.87\n",
      "\tState: (1, 9)\n",
      "\n",
      "\tState index: 19\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 37.56734\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.95432240925491\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 75, Score: 100\n",
      "\n",
      "\n",
      "Episode: 76\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.8362299311161\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.95889016832942\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 76, Score: 100\n",
      "\n",
      "\n",
      "Episode: 77\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.85360693800449\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.96300115149647\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 77, Score: 100\n",
      "\n",
      "\n",
      "Episode: 78\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.86924624420404\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.96670103634682\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 78, Score: 100\n",
      "\n",
      "\n",
      "Episode: 79\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.88332161978363\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.97003093271213\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 79, Score: 100\n",
      "\n",
      "\n",
      "Episode: 80\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.89598945780527\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.97302783944092\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 80, Score: 100\n",
      "\n",
      "\n",
      "Episode: 81\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -15.055273000000001\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.90739051202475\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.97572505549682\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 81, Score: 100\n",
      "\n",
      "\n",
      "Episode: 82\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.91765146082227\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.97815254994714\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 82, Score: 100\n",
      "\n",
      "\n",
      "Episode: 83\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.92688631474005\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.98033729495242\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 83, Score: 100\n",
      "\n",
      "\n",
      "Episode: 84\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tDirection: right\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -8.714350000000001\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.93519768326604\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.98230356545717\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 84, Score: 100\n",
      "\n",
      "\n",
      "Episode: 85\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.94267791493944\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.98407320891145\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 85, Score: 100\n",
      "\n",
      "\n",
      "Episode: 86\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.94941012344549\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.9856658880203\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 86, Score: 100\n",
      "\n",
      "\n",
      "Episode: 87\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.95546911110094\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.98709929921827\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 87, Score: 100\n",
      "\n",
      "\n",
      "Episode: 88\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.96092219999085\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.98838936929644\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 88, Score: 100\n",
      "\n",
      "\n",
      "Episode: 89\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.96582997999177\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.9895504323668\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 89, Score: 100\n",
      "\n",
      "\n",
      "Episode: 90\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.97024698199259\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99059538913012\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 90, Score: 100\n",
      "\n",
      "\n",
      "Episode: 91\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.97422228379332\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.9915358502171\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 91, Score: 100\n",
      "\n",
      "\n",
      "Episode: 92\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.977800055414\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99238226519539\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 92, Score: 100\n",
      "\n",
      "\n",
      "Episode: 93\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.9810200498726\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99314403867585\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 93, Score: 100\n",
      "\n",
      "\n",
      "Episode: 94\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.98391804488534\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99382963480826\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 94, Score: 100\n",
      "\n",
      "\n",
      "Episode: 95\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.98652624039681\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99444667132744\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 95, Score: 100\n",
      "\n",
      "\n",
      "Episode: 96\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.98887361635713\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99500200419469\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 96, Score: 100\n",
      "\n",
      "\n",
      "Episode: 97\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tDirection: down\n",
      "\tNew state: (1, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 10\n",
      "\tQ value: 2.5529209999999996\n",
      "\tState: (1, 0)\n",
      "\n",
      "\tState index: 10\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 27.006263\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.9909862547214\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99550180377523\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 97, Score: 100\n",
      "\n",
      "\n",
      "Episode: 98\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.99288762924927\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.9959516233977\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 98, Score: 100\n",
      "\n",
      "\n",
      "Episode: 99\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tDirection: left\n",
      "\tNew state: (0, 9)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 9\n",
      "\tQ value: 96.99459886632434\n",
      "\tState: (0, 9)\n",
      "\n",
      "\tState index: 9\n",
      "\tAction index: 0\n",
      "\tDirection: up\n",
      "\tNew state: (9, 9)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 99\n",
      "\tQ value: 99.99635646105793\n",
      "\tState: (9, 9)\n",
      "\n",
      "\tEpisode: 99, Score: 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m, n = laberinto.shape\n",
    "Q: np.ndarray = np.zeros((m * n, possible_actions))\n",
    "grid: tuple[int, int] = (m, n)\n",
    "obstacles: list[tuple[int, int]] = obstaculos\n",
    "\n",
    "for episode in range(episodes):\n",
    "    logger.info(f'Episode: {episode}')\n",
    "\n",
    "    state: tuple[int, int] = initial_state\n",
    "    logger.info(f'State: {state}\\n')\n",
    "\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    while not is_game_over:\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        logger.info(f'\\tState index: {state_index}')\n",
    "\n",
    "        action_index: int = choose_action(\n",
    "            Q,\n",
    "            state,\n",
    "            possible_actions,\n",
    "            grid,\n",
    "            epsilon\n",
    "        )\n",
    "        logger.info(f'\\tAction index: {action_index}')\n",
    "        logger.info(f'\\tDirection: {directions[action_index]}')\n",
    "        \n",
    "        new_state, reward, is_game_over = apply_action(\n",
    "            actions[action_index],\n",
    "            state,\n",
    "            goal_state,\n",
    "            grid,\n",
    "            obstacles\n",
    "        )\n",
    "        logger.info(f'\\tNew state: {new_state}')\n",
    "        logger.info(f'\\tReward: {reward}')\n",
    "        logger.info(f'\\tIs game over?: {is_game_over}')\n",
    "\n",
    "        new_state_index: int = convert_state_to_index(new_state, grid)\n",
    "        logger.info(f'\\tNew state index: {new_state_index}')\n",
    "\n",
    "        Q[state_index, action_index] = update_Q_values(\n",
    "            Q,\n",
    "            state_index,\n",
    "            action_index,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            new_state_index,\n",
    "            reward\n",
    "        )\n",
    "        logger.info(f'\\tQ value: {Q[state_index, action_index]}')\n",
    "\n",
    "        state = new_state\n",
    "        logger.info(f'\\tState: {state}\\n')\n",
    "\n",
    "        if is_game_over:\n",
    "            logger.info(f'\\tEpisode: {episode}, Score: {reward}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf32a0",
   "metadata": {},
   "source": [
    "##### 7. Función para mostrar el aprendizaje del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d41f03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it  # para los loops anidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e42f818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_learning(Q: np.ndarray, grid: tuple[int, int]) -> np.ndarray:\n",
    "    policy_matrix: np.ndarray = np.zeros(grid, dtype=int)\n",
    "\n",
    "    for i, j in it.product(*[range(cell) for cell in grid]):\n",
    "        state: tuple[int, int] = (i, j)\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        best_action: int = int(np.argmax(Q[state_index]))\n",
    "        policy_matrix[i, j] = best_action\n",
    "    \n",
    "    return policy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cb8db",
   "metadata": {},
   "source": [
    "##### 8. Visualizar el resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f1228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
