{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEGÃšN FEDERICO:**\n",
    "\n",
    "Es un tipo de algoritmo de aprendizaje por refuerzo que le permite a un agente imaginario aprender a tomar decisiones Ã³ptimas y a alcanzar un objetivo en ese entorno o ambiente determinado.\n",
    "\n",
    "El agente opera **aprendiendo los valores que sean mÃ¡s convenientes para cada paso** que tenga que dar, y a esos pasos los implementamos en un par de datos que definen la acciÃ³n que tiene que tomar y el estado en que queda.\n",
    "\n",
    "Entonces, Q-Learning implica un agente que debe identificar quÃ© pasos realizar y la **calidad de esos pasos** la vamos a denominar con la letra `Q`.\n",
    "\n",
    "Los valores `Q`, es decir, los valores de sus pasos que se definen por los valores de la acciÃ³n y del Estado, van a ayudar a este agente a decidir quÃ© acciÃ³n tiene que tomar en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EcuaciÃ³n de Bellman para actualizar valores Q\n",
    "\n",
    "![Bellman.png](./assets/Bellman.png)\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `ð‘„(ð‘ ,a)` = funciÃ³n ð‘„ que se estÃ¡ actualizando. Representa el valor actual estimado de tomar la acciÃ³n ð‘Ž en el estado ð‘ .\n",
    "\n",
    "- `ð‘ ` = estado (state)\n",
    "\n",
    "- `a` = acciÃ³n (action)\n",
    "\n",
    "- `Î±` = tasa de aprendizaje o **learning rate** (usualmente, un nÃºmero entre 0 y 1). Controla quÃ© tan rÃ¡pido actualizamos la estimaciÃ³n del valor. Si Î± es cercano a 1, significa que confiamos mucho en las nuevas experiencias, y si es cercano a 0, confiamos mÃ¡s en los valores previos. Dicho de otra manera, segÃºn Federico:\n",
    "\n",
    "    - Para un valor cercano a 1, el aprendizaje serÃ¡ mÃ¡s rÃ¡pido, pero menos seguro, con estimaciones menos estables para los valores Q.\n",
    "    \n",
    "    - Para un valor cercano a 0, el aprendizaje serÃ¡ mÃ¡s lento, pero mÃ¡s seguro en llevar una estimaciÃ³n mÃ¡s estable de los valores Q.\n",
    "\n",
    "- `R(ð‘ ,a)` = reforzador (reinforcer) o recompensa (reward), despuÃ©s de tomar la acciÃ³n ð‘Ž en el estado ð‘ . Esta recompensa indica el **beneficio obtenido** por realizar esa acciÃ³n en ese momento especÃ­fico.\n",
    "\n",
    "- `Î³` = factor de descuento o **discount factor**, tambiÃ©n un nÃºmero entre 0 y 1. Indica cuÃ¡nto valoramos las recompensas futuras. Un valor cercano a 1 significa que nos importan mucho las recompensas futuras (lo suficiente para considerar las consecuencias a largo plazo de nuestras acciones), mientras que un valor cercano a 0 significa que solo nos interesa la recompensa inmediata.\n",
    "\n",
    "- `ð‘ â€²` = nuevo estado al que llega el agente despuÃ©s de tomar la acciÃ³n ð‘Ž en el estado ð‘ .\n",
    "\n",
    "- `max aâ€² Q(ð‘ â€², aâ€²)` = selecciona el valor mÃ¡ximo de la funciÃ³n ð‘„ sobre todas las acciones posibles ð‘Žâ€² que el agente podrÃ­a tomar en el nuevo estado ð‘ â€². En otras palabras, el agente trata de estimar el mejor valor futuro posible en el siguiente estado, basÃ¡ndose en lo que ha aprendido hasta el momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de ejemplo\n",
    "\n",
    "Nos imaginemos un robot que opera en una cuadrÃ­cula que representa el salÃ³n de una fÃ¡brica en la que debe moverse trasladando herramientas.\n",
    "\n",
    "El objetivo es que el robot aprenda a encontrar el camino mÃ¡s corto, desde su posiciÃ³n inicial hasta el destino, evitando obstÃ¡culos.\n",
    "\n",
    "Entonces, el entorno que vamos a definir para este ejercicio va a ser una cuadrÃ­cula de dimensiones de 5x5, un punto de inicio en la esquina superior izquierda, que serÃ­a el punto `(0, 0)`, el punto objetivo en la esquina inferior derecha, que serÃ­a el punto `(4, 4)` y los obstÃ¡culos distribuidos en la cuadrÃ­cula.\n",
    "\n",
    "Vamos a establecer las acciones. Esto implica que el robot va a poder moverse en cuatro direcciones hacia arriba, hacia abajo, izquierda o derecha, y vamos a definir recompensas.\n",
    "\n",
    "Si alcanza el objetivo, va a tener 100 puntos de premio.\n",
    "\n",
    "Si colisiona con un obstÃ¡culo, se le van a restar 100 puntos y cualquier otro movimiento que haga va a valer -1.\n",
    "\n",
    "Â¿Para que?\n",
    "\n",
    "Para incentivar la eficiencia, que reste lo menos que pueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    '%(message)s'\n",
    ")\n",
    "\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid: tuple[int, int] = (5, 5)\n",
    "\n",
    "initial_state: tuple[int, int] = (0, 0)\n",
    "goal_state: tuple[int, int] = (4, 4)\n",
    "\n",
    "obstacles: list[tuple[int, int]] = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "\n",
    "up: int = 0\n",
    "down: int = 1\n",
    "left: int = 2\n",
    "right: int = 3\n",
    "\n",
    "actions: dict[int, tuple[int, int]] = {\n",
    "    up: (0, -1),\n",
    "    down: (0, 1),\n",
    "    left: (-1, 0),\n",
    "    right: (1, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_states: int = grid[0] * grid[1]\n",
    "grid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions: int = len(actions)\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table: np.ndarray = np.zeros((grid_states, possible_actions))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = convert_state_to_index((1, 0), grid)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor el resultado de `example`:\n",
    "\n",
    "![5x5_grid_example.png](./assets/5x5_grid_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Â¿CÃ³mo el agente robot va a aprender a navegar por el entorno?**\n",
    "\n",
    "Los siguientes parÃ¡metros son fundamentales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha: float = 0.1      # learning rate\n",
    "gamma: float = 0.99     # discount factor\n",
    "epsilon: float = 0.2    # exploration\n",
    "episodes: int = 100     # number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `epsilon` sirve para que el agente no repita siempre las mismas acciones, definiendo la **probabilidad** de que el agente tome una **acciÃ³n aleatoria** en lugar de la mejor acciÃ³n conocida hasta el momento, segÃºn la tabla Q.\n",
    "\n",
    "En otras palabras, esto permite que el agente **explore el entorno** desconocido en lugar de explotar constantemente el conocimiento que ya dispone, porque una vez que encuentre una soluciÃ³n, podrÃ­a tender a repetir esa soluciÃ³n siempre, en vez de explorar otras posibilidades (que podrÃ­an ser mÃ¡s eficientes o de mayor interÃ©s, segÃºn sea el caso).\n",
    "\n",
    "Un valor alto de `epsilon` (como **0.9**) significa que hay una **alta probabilidad** de que el agente tome una acciÃ³n aleatoria, promoviendo asÃ­ la **exploraciÃ³n**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `episodes` estÃ¡ definiendo el nÃºmero total de veces (episodios) que se va a repetir este proceso de entrenamiento.\n",
    "\n",
    "Un episodio comienza con el agente en el estado inicial (declarado en `initial_state`) y termina cuando alcanza el estado objetivo (declarado en `goal_state`) o algÃºn otro criterio de finalizaciÃ³n, segÃºn sea requerido.\n",
    "\n",
    "Mientras mayor sea el nÃºmero de episodios, mÃ¡s oportunidades va a tener el agente de tener mÃ¡s experiencias de las cuales aprender y esto va a mejorar potencialmente sus polÃ­ticas de acciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(\n",
    "        Q: np.ndarray,\n",
    "        state: tuple[int, int],\n",
    "        actions: dict[int, tuple[int, int]],\n",
    "        grid: tuple[int, int],\n",
    "        epsilon: float\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Choose a random action or the best action for the current state,\n",
    "    depending on the exploration rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "    \n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "    \n",
    "    actions : dict[int, tuple[int, int]]\n",
    "        The list of possible actions.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    epsilon : float\n",
    "        The exploration rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        A random action or the best action for the current state,\n",
    "        depending on the exploration rate.\n",
    "    \"\"\"\n",
    "    random_number: float = random.uniform(0, 1)\n",
    "    random_action: int = random.choice(list(actions.keys()))\n",
    "    best_action: int = int(np.argmax(Q[convert_state_to_index(state, grid)]))\n",
    "\n",
    "    if random_number < epsilon:\n",
    "        return random_action\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(\n",
    "        action: int,\n",
    "        state: tuple[int, int],\n",
    "        goal_state: tuple[int, int],\n",
    "        grid: tuple[int, int],\n",
    "        obstacles: list[tuple[int, int]]\n",
    "        ) -> tuple[tuple[int, int], int, bool]:\n",
    "    \"\"\"\n",
    "    Apply the action to the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    action : int\n",
    "        The action to be applied.\n",
    "\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    goal_state : tuple[int, int]\n",
    "        The two-dimensional representation of the state declared as\n",
    "        goal.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    obstacles : list[tuple[int, int]]\n",
    "        The list of obstacles on the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple[int, int], int, bool]\n",
    "        The new state, the reward, and whether the game is over.\n",
    "\n",
    "    \"\"\"\n",
    "    points: int = 0\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    new_state: tuple[int, int] = tuple(np.add(state, action) % grid)\n",
    "\n",
    "    if new_state in obstacles or new_state == state:\n",
    "        points = -100\n",
    "        return state, points, is_game_over\n",
    "    elif new_state == goal_state:\n",
    "        points = 100\n",
    "        is_game_over = True\n",
    "        return new_state, points, is_game_over\n",
    "    else:\n",
    "        points = -1\n",
    "        return new_state, points, is_game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_values(\n",
    "        Q: np.ndarray,\n",
    "        state_index: int,\n",
    "        action_index: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        new_state_index: int,\n",
    "        reward: int,\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    Update the Q values for the current state and action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "\n",
    "    state_index : tuple[int, int]\n",
    "        The index of the current state in the Q table.\n",
    "\n",
    "    action_index : int\n",
    "        The index of the current action in the Q table.\n",
    "\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "\n",
    "    gamma : float\n",
    "        The discount factor.\n",
    "\n",
    "    new_state_index : tuple[int, int]\n",
    "        The index of the new state in the Q table.\n",
    "\n",
    "    reward : int\n",
    "        The reward for the current state and action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The updated Q value.\n",
    "    \"\"\"\n",
    "    current_q = Q[state_index, action_index]\n",
    "    best_future_q = np.max(Q[new_state_index])\n",
    "    return current_q + alpha * (reward + (gamma * best_future_q) - current_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 1\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -10.0\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 10.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 0, Score: 100\n",
      "\n",
      "\n",
      "Episode: 1\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -19.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 0.8900000000000001\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 19.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 1, Score: 100\n",
      "\n",
      "\n",
      "Episode: 2\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 2.582\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 27.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 2, Score: 100\n",
      "\n",
      "\n",
      "Episode: 3\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 4.9067\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 34.39\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 3, Score: 100\n",
      "\n",
      "\n",
      "Episode: 4\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 7.72064\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 40.951\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 4, Score: 100\n",
      "\n",
      "\n",
      "Episode: 5\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.19\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 0\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -10.0\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 10.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 5, Score: 100\n",
      "\n",
      "\n",
      "Episode: 6\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 10.902725\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 46.8559\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 6, Score: 100\n",
      "\n",
      "\n",
      "Episode: 7\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 14.3511866\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 52.17031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 7, Score: 100\n",
      "\n",
      "\n",
      "Episode: 8\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 17.98092863\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 56.953279\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 8, Score: 100\n",
      "\n",
      "\n",
      "Episode: 9\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 21.721210388000003\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 61.2579511\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 9, Score: 100\n",
      "\n",
      "\n",
      "Episode: 10\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 0.7190000000000001\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 19.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 10, Score: 100\n",
      "\n",
      "\n",
      "Episode: 11\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 25.513626508100003\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 65.13215599\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 11, Score: 100\n",
      "\n",
      "\n",
      "Episode: 12\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 29.310347300300002\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 68.618940391\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 12, Score: 100\n",
      "\n",
      "\n",
      "Episode: 13\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 33.072587668979\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 71.7570463519\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 13, Score: 100\n",
      "\n",
      "\n",
      "Episode: 14\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 36.7692764909192\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 74.58134171671\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 14, Score: 100\n",
      "\n",
      "\n",
      "Episode: 15\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 2.4280999999999997\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 27.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 15, Score: 100\n",
      "\n",
      "\n",
      "Episode: 16\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 40.37590167178157\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 77.123207545039\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 16, Score: 100\n",
      "\n",
      "\n",
      "Episode: 17\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 43.87350905156227\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 79.4108867905351\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 17, Score: 100\n",
      "\n",
      "\n",
      "Episode: 18\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 47.24783593866902\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 81.4697981114816\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 18, Score: 100\n",
      "\n",
      "\n",
      "Episode: 19\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 50.488562357838795\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 83.32281830033344\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 19, Score: 100\n",
      "\n",
      "\n",
      "Episode: 20\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 53.58866513378793\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -10.75104098826699\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 84.9905364703001\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 20, Score: 100\n",
      "\n",
      "\n",
      "Episode: 21\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 56.54386173096884\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 86.49148282327009\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 21, Score: 100\n",
      "\n",
      "\n",
      "Episode: 22\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 59.352132357375694\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -1.437343200496261\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 87.84233454094309\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 22, Score: 100\n",
      "\n",
      "\n",
      "Episode: 23\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 62.01331024119149\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 89.05810108684878\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 23, Score: 100\n",
      "\n",
      "\n",
      "Episode: 24\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -20.960682286122044\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 64.52873122467037\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 90.1522909781639\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 24, Score: 100\n",
      "\n",
      "\n",
      "Episode: 25\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -12.611655608757633\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 66.90093490904155\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 91.13706188034752\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 25, Score: 100\n",
      "\n",
      "\n",
      "Episode: 26\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 69.1334105442918\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -2.2710397542922305\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -3.021366652708603\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 92.02335569231276\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 26, Score: 100\n",
      "\n",
      "\n",
      "Episode: 27\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -22.02040641362495\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 71.23038170340159\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 92.82102012308148\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 27, Score: 100\n",
      "\n",
      "\n",
      "Episode: 28\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 73.1966245252465\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 93.53891811077334\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 28, Score: 100\n",
      "\n",
      "\n",
      "Episode: 29\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 75.03731496568841\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 94.185026299696\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 29, Score: 100\n",
      "\n",
      "\n",
      "Episode: 30\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 76.75790107278948\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 94.76652366972641\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 30, Score: 100\n",
      "\n",
      "\n",
      "Episode: 31\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 78.36399680881345\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 95.28987130275377\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 31, Score: 100\n",
      "\n",
      "\n",
      "Episode: 32\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 79.86129438690473\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 95.76088417247838\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 32, Score: 100\n",
      "\n",
      "\n",
      "Episode: 33\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 81.25549248128962\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.18479575523054\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 33, Score: 100\n",
      "\n",
      "\n",
      "Episode: 34\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 82.55223801292848\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 8.07267156327992\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 83.71930899140347\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.56631617970748\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 34, Score: 100\n",
      "\n",
      "\n",
      "Episode: 35\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 84.80744339405416\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.90968456173674\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 35, Score: 100\n",
      "\n",
      "\n",
      "Episode: 36\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 85.82075782626069\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.21871610556306\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 36, Score: 100\n",
      "\n",
      "\n",
      "Episode: 37\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 4.76819\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 0\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -16.3171\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 34.39\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 37, Score: 100\n",
      "\n",
      "\n",
      "Episode: 38\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 86.76333493808536\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.49684449500675\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 38, Score: 100\n",
      "\n",
      "\n",
      "Episode: 39\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 7.595981\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 40.951\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 39, Score: 100\n",
      "\n",
      "\n",
      "Episode: 40\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 87.63918904928249\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.74716004550608\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 40, Score: 100\n",
      "\n",
      "\n",
      "Episode: 41\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 88.45223898885935\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -3.042261142932641\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.97244404095547\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 41, Score: 100\n",
      "\n",
      "\n",
      "Episode: 42\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -21.06159411236538\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 89.20628705002801\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.17519963685992\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 42, Score: 100\n",
      "\n",
      "\n",
      "Episode: 43\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 89.90500310907434\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.35767967317392\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 43, Score: 100\n",
      "\n",
      "\n",
      "Episode: 44\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 90.55191308581112\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -9.938526601796072\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.52191170585652\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 44, Score: 100\n",
      "\n",
      "\n",
      "Episode: 45\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 91.15039103610981\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -9.191004682736668\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.66972053527087\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 45, Score: 100\n",
      "\n",
      "\n",
      "Episode: 46\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 91.70365426549064\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.80274848174378\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 46, Score: 100\n",
      "\n",
      "\n",
      "Episode: 47\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 92.21476093863421\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.9224736335694\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 47, Score: 100\n",
      "\n",
      "\n",
      "Episode: 48\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 92.68660973449417\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -8.478579324739632\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.03022627021245\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 48, Score: 100\n",
      "\n",
      "\n",
      "Episode: 49\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.12194116179579\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 16.38447658196971\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.51373944636724\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -7.826728991514637\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.12720364319121\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 49, Score: 100\n",
      "\n",
      "\n",
      "Episode: 50\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.87595866240645\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 23.93974883135098\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.20195595684173\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.21448327887208\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 50, Score: 100\n",
      "\n",
      "\n",
      "Episode: 51\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.50399420576589\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.29303495098488\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 51, Score: 100\n",
      "\n",
      "\n",
      "Episode: 52\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.7836052453368\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.36373145588638\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 52, Score: 100\n",
      "\n",
      "\n",
      "Episode: 53\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.04225413493587\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.42735831029775\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 53, Score: 100\n",
      "\n",
      "\n",
      "Episode: 54\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.28133719416176\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.48462247926797\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 54, Score: 100\n",
      "\n",
      "\n",
      "Episode: 55\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.50218110019311\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.53616023134117\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 55, Score: 100\n",
      "\n",
      "\n",
      "Episode: 56\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.70604285307658\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.58254420820705\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 56, Score: 100\n",
      "\n",
      "\n",
      "Episode: 57\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.89411044438143\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -7.185384215750674\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.62428978738635\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 57, Score: 100\n",
      "\n",
      "\n",
      "Episode: 58\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.06750408889454\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.66186080864772\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 58, Score: 100\n",
      "\n",
      "\n",
      "Episode: 59\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -19.444751796328283\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.22727790006121\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -2.8715108085832535\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.69567472778294\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 59, Score: 100\n",
      "\n",
      "\n",
      "Episode: 60\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.3744219081056\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.72610725500465\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 60, Score: 100\n",
      "\n",
      "\n",
      "Episode: 61\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -11.809422278979415\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.5098643355405\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.75349652950419\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 61, Score: 100\n",
      "\n",
      "\n",
      "Episode: 62\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.63447405840736\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.77814687655378\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 62, Score: 100\n",
      "\n",
      "\n",
      "Episode: 63\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.74906319334545\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8003321888984\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 63, Score: 100\n",
      "\n",
      "\n",
      "Episode: 64\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.85438976071184\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.82029897000855\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 64, Score: 100\n",
      "\n",
      "\n",
      "Episode: 65\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.9511603826715\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8382690730077\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 65, Score: 100\n",
      "\n",
      "\n",
      "Episode: 66\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.04003298263211\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 31.05273721349646\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.12001832259666\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.85444216570693\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 66, Score: 100\n",
      "\n",
      "\n",
      "Episode: 67\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -17.885394802758388\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.19360626474199\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.86899794913623\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 67, Score: 100\n",
      "\n",
      "\n",
      "Episode: 68\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.26127643523228\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8820981542226\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 68, Score: 100\n",
      "\n",
      "\n",
      "Episode: 69\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.32347650897708\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.89388833880034\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 69, Score: 100\n",
      "\n",
      "\n",
      "Episode: 70\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.38062380362061\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.90449950492031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 70, Score: 100\n",
      "\n",
      "\n",
      "Episode: 71\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.43310687424565\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.91404955442827\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 71, Score: 100\n",
      "\n",
      "\n",
      "Episode: 72\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.4812870927095\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.92264459898544\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 72, Score: 100\n",
      "\n",
      "\n",
      "Episode: 73\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.5255001987381\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.93038013908689\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 73, Score: 100\n",
      "\n",
      "\n",
      "Episode: 74\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.56605781263389\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9373421251782\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 74, Score: 100\n",
      "\n",
      "\n",
      "Episode: 75\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.60324890176314\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 37.510185133421366\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.63672088197947\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.94360791266038\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 75, Score: 100\n",
      "\n",
      "\n",
      "Episode: 76\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.6674659771349\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.94924712139435\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 76, Score: 100\n",
      "\n",
      "\n",
      "Episode: 77\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.69569484443944\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.95432240925491\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 77, Score: 100\n",
      "\n",
      "\n",
      "Episode: 78\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.72160327851174\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.95889016832942\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 78, Score: 100\n",
      "\n",
      "\n",
      "Episode: 79\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.74537307732517\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -6.570915667510994\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.96300115149647\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 79, Score: 100\n",
      "\n",
      "\n",
      "Episode: 80\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -16.420063387827355\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.7671728835908\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.96670103634682\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 80, Score: 100\n",
      "\n",
      "\n",
      "Episode: 81\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.78715899783006\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 43.3400953608644\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.80514650064539\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97003093271213\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 81, Score: 100\n",
      "\n",
      "\n",
      "Episode: 82\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.82166491291936\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 48.59043065115698\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.83653148396593\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97302783944092\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 82, Score: 100\n",
      "\n",
      "\n",
      "Episode: 83\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.85020809167399\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97572505549682\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 83, Score: 100\n",
      "\n",
      "\n",
      "Episode: 84\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.86278406300077\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97815254994714\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 84, Score: 100\n",
      "\n",
      "\n",
      "Episode: 85\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.87434275914546\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98033729495242\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 85, Score: 100\n",
      "\n",
      "\n",
      "Episode: 86\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.8849618754312\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98230356545717\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 86, Score: 100\n",
      "\n",
      "\n",
      "Episode: 87\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.89471374086834\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98407320891145\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 87, Score: 100\n",
      "\n",
      "\n",
      "Episode: 88\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -15.086480388698654\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.90366561446373\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9856658880203\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 88, Score: 100\n",
      "\n",
      "\n",
      "Episode: 89\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.91187997593137\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -6.015243177845885\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98709929921827\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 89, Score: 100\n",
      "\n",
      "\n",
      "Episode: 90\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.935203933464269\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.91941480896084\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98838936929644\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 90, Score: 100\n",
      "\n",
      "\n",
      "Episode: 91\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.9263238756251\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9895504323668\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 91, Score: 100\n",
      "\n",
      "\n",
      "Episode: 92\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.93265698086691\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99059538913012\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 92, Score: 100\n",
      "\n",
      "\n",
      "Episode: 93\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.9384602263041\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9915358502171\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 93, Score: 100\n",
      "\n",
      "\n",
      "Episode: 94\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.94377625284518\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99238226519539\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 94, Score: 100\n",
      "\n",
      "\n",
      "Episode: 95\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -13.881398500797115\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -12.79682480168573\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 10.790531900000001\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 46.8559\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 95, Score: 100\n",
      "\n",
      "\n",
      "Episode: 96\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 14.25021281\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 52.17031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 96, Score: 100\n",
      "\n",
      "\n",
      "Episode: 97\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.94864447181502\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99314403867585\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 97, Score: 100\n",
      "\n",
      "\n",
      "Episode: 98\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 17.890052219\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 56.953279\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 98, Score: 100\n",
      "\n",
      "\n",
      "Episode: 99\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -11.82022651880747\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.95310128446242\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99382963480826\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 99, Score: 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    logger.info(f'Episode: {episode}')\n",
    "    state: tuple[int, int] = initial_state\n",
    "    logger.info(f'State: {state}\\n')\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    while not is_game_over:\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        logger.info(f'\\tState index: {state_index}')\n",
    "        action_index: int = choose_action(\n",
    "            q_table,\n",
    "            state,\n",
    "            actions,\n",
    "            grid,\n",
    "            epsilon\n",
    "        )\n",
    "        logger.info(f'\\tAction index: {action_index}')\n",
    "        new_state, reward, is_game_over = apply_action(\n",
    "            action_index,\n",
    "            state,\n",
    "            goal_state,\n",
    "            grid,\n",
    "            obstacles\n",
    "        )\n",
    "        logger.info(f'\\tNew state: {new_state}')\n",
    "        logger.info(f'\\tReward: {reward}')\n",
    "        logger.info(f'\\tIs game over?: {is_game_over}')\n",
    "        new_state_index: int = convert_state_to_index(new_state, grid)\n",
    "        logger.info(f'\\tNew state index: {new_state_index}')\n",
    "\n",
    "        q_table[state_index, action_index] = update_Q_values(\n",
    "            q_table,\n",
    "            state_index,\n",
    "            action_index,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            new_state_index,\n",
    "            reward\n",
    "        )\n",
    "        logger.info(f'\\tQ value: {q_table[state_index, action_index]}')\n",
    "\n",
    "        state = new_state\n",
    "        logger.info(f'\\tState: {state}\\n')\n",
    "\n",
    "        if is_game_over:\n",
    "            logger.info(f'\\tEpisode: {episode}, Score: {reward}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar el proceso de una manera mÃ¡s grÃ¡fica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **matriz politica** contiene la **mejor acciÃ³n** que el agente debe tomar **en cada estado** basado en el conocimiento adquirido durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_matrix: np.ndarray = np.zeros(grid, dtype=int)\n",
    "policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "### PolÃ­tica aprendida ###\n",
      "\n",
      "[[3 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 2 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "\n",
      "Leyenda:\n",
      "0: arriba\n",
      "1: abajo\n",
      "2: izquierda\n",
      "3: derecha\n"
     ]
    }
   ],
   "source": [
    "# Llenar la matriz con la mejor acciÃ³n posible para cada estado\n",
    "for i in range(grid[0]):\n",
    "    for j in range(grid[1]):\n",
    "        state: tuple[int, int] = (i, j)\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        best_action: int = int(np.argmax(q_table[state_index]))\n",
    "        policy_matrix[i, j] = best_action\n",
    "\n",
    "logger.info('### PolÃ­tica aprendida ###\\n')\n",
    "logger.info(policy_matrix)\n",
    "logger.info('\\nLeyenda:')\n",
    "logger.info('0: arriba\\n1: abajo\\n2: izquierda\\n3: derecha')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
