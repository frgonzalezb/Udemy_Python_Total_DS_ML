{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEGÚN FEDERICO:**\n",
    "\n",
    "Es un tipo de algoritmo de aprendizaje por refuerzo que le permite a un agente imaginario aprender a tomar decisiones óptimas y a alcanzar un objetivo en ese entorno o ambiente determinado.\n",
    "\n",
    "El agente opera **aprendiendo los valores que sean más convenientes para cada paso** que tenga que dar, y a esos pasos los implementamos en un par de datos que definen la acción que tiene que tomar y el estado en que queda.\n",
    "\n",
    "Entonces, Q-Learning implica un agente que debe identificar qué pasos realizar y la **calidad de esos pasos** la vamos a denominar con la letra `Q`.\n",
    "\n",
    "Los valores `Q`, es decir, los valores de sus pasos que se definen por los valores de la acción y del Estado, van a ayudar a este agente a decidir qué acción tiene que tomar en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de Bellman para actualizar valores Q\n",
    "\n",
    "![Bellman.png](./assets/Bellman.png)\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `𝑄(𝑠,a)` = función 𝑄 que se está actualizando. Representa el valor actual estimado de tomar la acción 𝑎 en el estado 𝑠.\n",
    "\n",
    "- `𝑠` = estado (state)\n",
    "\n",
    "- `a` = acción (action)\n",
    "\n",
    "- `α` = tasa de aprendizaje o **learning rate** (usualmente, un número entre 0 y 1). Controla qué tan rápido actualizamos la estimación del valor. Si α es cercano a 1, significa que confiamos mucho en las nuevas experiencias, y si es cercano a 0, confiamos más en los valores previos. Dicho de otra manera, según Federico:\n",
    "\n",
    "    - Para un valor cercano a 1, el aprendizaje será más rápido, pero menos seguro, con estimaciones menos estables para los valores Q.\n",
    "    \n",
    "    - Para un valor cercano a 0, el aprendizaje será más lento, pero más seguro en llevar una estimación más estable de los valores Q.\n",
    "\n",
    "- `R(𝑠,a)` = reforzador (reinforcer) o recompensa (reward), después de tomar la acción 𝑎 en el estado 𝑠. Esta recompensa indica el **beneficio obtenido** por realizar esa acción en ese momento específico.\n",
    "\n",
    "- `γ` = factor de descuento o **discount factor**, también un número entre 0 y 1. Indica cuánto valoramos las recompensas futuras. Un valor cercano a 1 significa que nos importan mucho las recompensas futuras (lo suficiente para considerar las consecuencias a largo plazo de nuestras acciones), mientras que un valor cercano a 0 significa que solo nos interesa la recompensa inmediata.\n",
    "\n",
    "- `𝑠′` = nuevo estado al que llega el agente después de tomar la acción 𝑎 en el estado 𝑠.\n",
    "\n",
    "- `max a′ Q(𝑠′, a′)` = selecciona el valor máximo de la función 𝑄 sobre todas las acciones posibles 𝑎′ que el agente podría tomar en el nuevo estado 𝑠′. En otras palabras, el agente trata de estimar el mejor valor futuro posible en el siguiente estado, basándose en lo que ha aprendido hasta el momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de ejemplo\n",
    "\n",
    "Nos imaginemos un robot que opera en una cuadrícula que representa el salón de una fábrica en la que debe moverse trasladando herramientas.\n",
    "\n",
    "El objetivo es que el robot aprenda a encontrar el camino más corto, desde su posición inicial hasta el destino, evitando obstáculos.\n",
    "\n",
    "Entonces, el entorno que vamos a definir para este ejercicio va a ser una cuadrícula de dimensiones de 5x5, un punto de inicio en la esquina superior izquierda, que sería el punto `(0, 0)`, el punto objetivo en la esquina inferior derecha, que sería el punto `(4, 4)` y los obstáculos distribuidos en la cuadrícula.\n",
    "\n",
    "Vamos a establecer las acciones. Esto implica que el robot va a poder moverse en cuatro direcciones hacia arriba, hacia abajo, izquierda o derecha, y vamos a definir recompensas.\n",
    "\n",
    "Si alcanza el objetivo, va a tener 100 puntos de premio.\n",
    "\n",
    "Si colisiona con un obstáculo, se le van a restar 100 puntos y cualquier otro movimiento que haga va a valer -1.\n",
    "\n",
    "¿Para que?\n",
    "\n",
    "Para incentivar la eficiencia, que reste lo menos que pueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "console_handler: logging.StreamHandler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter: logging.Formatter = logging.Formatter(\n",
    "    '%(message)s'\n",
    ")\n",
    "\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid: tuple[int, int] = (5, 5)\n",
    "\n",
    "initial_state: tuple[int, int] = (0, 0)\n",
    "goal_state: tuple[int, int] = (4, 4)\n",
    "\n",
    "obstacles: list[tuple[int, int]] = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "\n",
    "up: int = 0\n",
    "down: int = 1\n",
    "left: int = 2\n",
    "right: int = 3\n",
    "\n",
    "actions: dict[int, tuple[int, int]] = {\n",
    "    up: (0, -1),\n",
    "    down: (0, 1),\n",
    "    left: (-1, 0),\n",
    "    right: (1, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_states: int = grid[0] * grid[1]\n",
    "grid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions: int = len(actions)\n",
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table: np.ndarray = np.zeros((grid_states, possible_actions))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = convert_state_to_index((1, 0), grid)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor el resultado de `example`:\n",
    "\n",
    "![5x5_grid_example.png](./assets/5x5_grid_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Cómo el agente robot va a aprender a navegar por el entorno?**\n",
    "\n",
    "Los siguientes parámetros son fundamentales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha: float = 0.1      # learning rate\n",
    "gamma: float = 0.99     # discount factor\n",
    "epsilon: float = 0.2    # exploration\n",
    "episodes: int = 100     # number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `epsilon` sirve para que el agente no repita siempre las mismas acciones, definiendo la **probabilidad** de que el agente tome una **acción aleatoria** en lugar de la mejor acción conocida hasta el momento, según la tabla Q.\n",
    "\n",
    "En otras palabras, esto permite que el agente **explore el entorno** desconocido en lugar de explotar constantemente el conocimiento que ya dispone, porque una vez que encuentre una solución, podría tender a repetir esa solución siempre, en vez de explorar otras posibilidades (que podrían ser más eficientes o de mayor interés, según sea el caso).\n",
    "\n",
    "Un valor alto de `epsilon` (como **0.9**) significa que hay una **alta probabilidad** de que el agente tome una acción aleatoria, promoviendo así la **exploración**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `episodes` está definiendo el número total de veces (episodios) que se va a repetir este proceso de entrenamiento.\n",
    "\n",
    "Un episodio comienza con el agente en el estado inicial (declarado en `initial_state`) y termina cuando alcanza el estado objetivo (declarado en `goal_state`) o algún otro criterio de finalización, según sea requerido.\n",
    "\n",
    "Mientras mayor sea el número de episodios, más oportunidades va a tener el agente de tener más experiencias de las cuales aprender y esto va a mejorar potencialmente sus políticas de acción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(\n",
    "        Q: np.ndarray,\n",
    "        state: tuple[int, int],\n",
    "        actions: dict[int, tuple[int, int]],\n",
    "        grid: tuple[int, int],\n",
    "        epsilon: float\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Choose a random action or the best action for the current state,\n",
    "    depending on the exploration rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "    \n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "    \n",
    "    actions : dict[int, tuple[int, int]]\n",
    "        The list of possible actions.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    epsilon : float\n",
    "        The exploration rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        A random action or the best action for the current state,\n",
    "        depending on the exploration rate.\n",
    "    \"\"\"\n",
    "    random_number: float = random.uniform(0, 1)\n",
    "    random_action: int = random.choice(list(actions.keys()))\n",
    "    best_action: int = int(np.argmax(Q[convert_state_to_index(state, grid)]))\n",
    "\n",
    "    if random_number < epsilon:\n",
    "        return random_action\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(\n",
    "        action: int,\n",
    "        state: tuple[int, int],\n",
    "        goal_state: tuple[int, int],\n",
    "        grid: tuple[int, int],\n",
    "        obstacles: list[tuple[int, int]]\n",
    "        ) -> tuple[tuple[int, int], int, bool]:\n",
    "    \"\"\"\n",
    "    Apply the action to the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    action : int\n",
    "        The action to be applied.\n",
    "\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    goal_state : tuple[int, int]\n",
    "        The two-dimensional representation of the state declared as\n",
    "        goal.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    obstacles : list[tuple[int, int]]\n",
    "        The list of obstacles on the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple[int, int], int, bool]\n",
    "        The new state, the reward, and whether the game is over.\n",
    "\n",
    "    \"\"\"\n",
    "    points: int = 0\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    new_state: tuple[int, int] = tuple(np.add(state, action) % grid)\n",
    "\n",
    "    if new_state in obstacles or new_state == state:\n",
    "        points = -100\n",
    "        return state, points, is_game_over\n",
    "    elif new_state == goal_state:\n",
    "        points = 100\n",
    "        is_game_over = True\n",
    "        return new_state, points, is_game_over\n",
    "    else:\n",
    "        points = -1\n",
    "        return new_state, points, is_game_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_values(\n",
    "        Q: np.ndarray,\n",
    "        state_index: int,\n",
    "        action_index: int,\n",
    "        alpha: float,\n",
    "        gamma: float,\n",
    "        new_state_index: int,\n",
    "        reward: int,\n",
    "        ) -> float:\n",
    "    \"\"\"\n",
    "    Update the Q values for the current state and action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "\n",
    "    state_index : tuple[int, int]\n",
    "        The index of the current state in the Q table.\n",
    "\n",
    "    action_index : int\n",
    "        The index of the current action in the Q table.\n",
    "\n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "\n",
    "    gamma : float\n",
    "        The discount factor.\n",
    "\n",
    "    new_state_index : tuple[int, int]\n",
    "        The index of the new state in the Q table.\n",
    "\n",
    "    reward : int\n",
    "        The reward for the current state and action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The updated Q value.\n",
    "    \"\"\"\n",
    "    current_q = Q[state_index, action_index]\n",
    "    best_future_q = np.max(Q[new_state_index])\n",
    "    return current_q + alpha * (reward + (gamma * best_future_q) - current_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.1\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 1\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -0.1\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -10.0\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 10.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 0, Score: 100\n",
      "\n",
      "\n",
      "Episode: 1\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -19.0\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 0.8900000000000001\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 19.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 1, Score: 100\n",
      "\n",
      "\n",
      "Episode: 2\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 2.582\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 27.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 2, Score: 100\n",
      "\n",
      "\n",
      "Episode: 3\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 4.9067\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 34.39\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 3, Score: 100\n",
      "\n",
      "\n",
      "Episode: 4\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 7.72064\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 40.951\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 4, Score: 100\n",
      "\n",
      "\n",
      "Episode: 5\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -0.19\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 0\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -10.0\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 10.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 5, Score: 100\n",
      "\n",
      "\n",
      "Episode: 6\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 10.902725\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 46.8559\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 6, Score: 100\n",
      "\n",
      "\n",
      "Episode: 7\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 14.3511866\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 52.17031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 7, Score: 100\n",
      "\n",
      "\n",
      "Episode: 8\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 17.98092863\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 56.953279\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 8, Score: 100\n",
      "\n",
      "\n",
      "Episode: 9\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 21.721210388000003\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 61.2579511\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 9, Score: 100\n",
      "\n",
      "\n",
      "Episode: 10\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 0.7190000000000001\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 19.0\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 10, Score: 100\n",
      "\n",
      "\n",
      "Episode: 11\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 25.513626508100003\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 65.13215599\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 11, Score: 100\n",
      "\n",
      "\n",
      "Episode: 12\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 29.310347300300002\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 68.618940391\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 12, Score: 100\n",
      "\n",
      "\n",
      "Episode: 13\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 33.072587668979\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 71.7570463519\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 13, Score: 100\n",
      "\n",
      "\n",
      "Episode: 14\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 36.7692764909192\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 74.58134171671\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 14, Score: 100\n",
      "\n",
      "\n",
      "Episode: 15\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 2.4280999999999997\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 27.1\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 15, Score: 100\n",
      "\n",
      "\n",
      "Episode: 16\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 40.37590167178157\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 77.123207545039\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 16, Score: 100\n",
      "\n",
      "\n",
      "Episode: 17\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 43.87350905156227\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 79.4108867905351\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 17, Score: 100\n",
      "\n",
      "\n",
      "Episode: 18\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 47.24783593866902\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 81.4697981114816\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 18, Score: 100\n",
      "\n",
      "\n",
      "Episode: 19\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 50.488562357838795\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 83.32281830033344\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 19, Score: 100\n",
      "\n",
      "\n",
      "Episode: 20\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 53.58866513378793\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -10.75104098826699\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 84.9905364703001\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 20, Score: 100\n",
      "\n",
      "\n",
      "Episode: 21\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 56.54386173096884\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 86.49148282327009\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 21, Score: 100\n",
      "\n",
      "\n",
      "Episode: 22\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 59.352132357375694\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -1.437343200496261\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 87.84233454094309\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 22, Score: 100\n",
      "\n",
      "\n",
      "Episode: 23\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 62.01331024119149\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 89.05810108684878\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 23, Score: 100\n",
      "\n",
      "\n",
      "Episode: 24\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -20.960682286122044\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 64.52873122467037\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 90.1522909781639\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 24, Score: 100\n",
      "\n",
      "\n",
      "Episode: 25\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -12.611655608757633\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 66.90093490904155\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 91.13706188034752\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 25, Score: 100\n",
      "\n",
      "\n",
      "Episode: 26\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 69.1334105442918\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -2.2710397542922305\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -3.021366652708603\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 92.02335569231276\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 26, Score: 100\n",
      "\n",
      "\n",
      "Episode: 27\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -22.02040641362495\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 71.23038170340159\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 92.82102012308148\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 27, Score: 100\n",
      "\n",
      "\n",
      "Episode: 28\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 73.1966245252465\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 93.53891811077334\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 28, Score: 100\n",
      "\n",
      "\n",
      "Episode: 29\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 75.03731496568841\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 94.185026299696\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 29, Score: 100\n",
      "\n",
      "\n",
      "Episode: 30\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 76.75790107278948\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 94.76652366972641\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 30, Score: 100\n",
      "\n",
      "\n",
      "Episode: 31\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 78.36399680881345\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 95.28987130275377\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 31, Score: 100\n",
      "\n",
      "\n",
      "Episode: 32\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 79.86129438690473\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 95.76088417247838\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 32, Score: 100\n",
      "\n",
      "\n",
      "Episode: 33\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 81.25549248128962\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.18479575523054\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 33, Score: 100\n",
      "\n",
      "\n",
      "Episode: 34\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 82.55223801292848\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 8.07267156327992\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 83.71930899140347\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.56631617970748\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 34, Score: 100\n",
      "\n",
      "\n",
      "Episode: 35\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 84.80744339405416\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 96.90968456173674\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 35, Score: 100\n",
      "\n",
      "\n",
      "Episode: 36\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 85.82075782626069\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.21871610556306\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 36, Score: 100\n",
      "\n",
      "\n",
      "Episode: 37\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 4.76819\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 0\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: -16.3171\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 34.39\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 37, Score: 100\n",
      "\n",
      "\n",
      "Episode: 38\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 86.76333493808536\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.49684449500675\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 38, Score: 100\n",
      "\n",
      "\n",
      "Episode: 39\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 7.595981\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 40.951\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 39, Score: 100\n",
      "\n",
      "\n",
      "Episode: 40\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 87.63918904928249\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.74716004550608\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 40, Score: 100\n",
      "\n",
      "\n",
      "Episode: 41\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 88.45223898885935\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -3.042261142932641\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 97.97244404095547\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 41, Score: 100\n",
      "\n",
      "\n",
      "Episode: 42\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -21.06159411236538\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 89.20628705002801\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.17519963685992\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 42, Score: 100\n",
      "\n",
      "\n",
      "Episode: 43\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 89.90500310907434\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.35767967317392\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 43, Score: 100\n",
      "\n",
      "\n",
      "Episode: 44\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 90.55191308581112\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -9.938526601796072\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.52191170585652\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 44, Score: 100\n",
      "\n",
      "\n",
      "Episode: 45\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 91.15039103610981\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -9.191004682736668\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.66972053527087\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 45, Score: 100\n",
      "\n",
      "\n",
      "Episode: 46\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 91.70365426549064\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.80274848174378\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 46, Score: 100\n",
      "\n",
      "\n",
      "Episode: 47\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 92.21476093863421\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 98.9224736335694\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 47, Score: 100\n",
      "\n",
      "\n",
      "Episode: 48\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 92.68660973449417\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -8.478579324739632\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.03022627021245\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 48, Score: 100\n",
      "\n",
      "\n",
      "Episode: 49\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.12194116179579\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 16.38447658196971\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.51373944636724\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -7.826728991514637\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.12720364319121\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 49, Score: 100\n",
      "\n",
      "\n",
      "Episode: 50\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 93.87595866240645\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 23.93974883135098\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.20195595684173\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.21448327887208\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 50, Score: 100\n",
      "\n",
      "\n",
      "Episode: 51\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.50399420576589\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.29303495098488\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 51, Score: 100\n",
      "\n",
      "\n",
      "Episode: 52\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 94.7836052453368\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.36373145588638\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 52, Score: 100\n",
      "\n",
      "\n",
      "Episode: 53\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.04225413493587\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.42735831029775\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 53, Score: 100\n",
      "\n",
      "\n",
      "Episode: 54\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.28133719416176\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.48462247926797\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 54, Score: 100\n",
      "\n",
      "\n",
      "Episode: 55\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.50218110019311\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.53616023134117\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 55, Score: 100\n",
      "\n",
      "\n",
      "Episode: 56\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.70604285307658\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.58254420820705\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 56, Score: 100\n",
      "\n",
      "\n",
      "Episode: 57\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 95.89411044438143\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -7.185384215750674\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.62428978738635\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 57, Score: 100\n",
      "\n",
      "\n",
      "Episode: 58\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.06750408889454\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.66186080864772\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 58, Score: 100\n",
      "\n",
      "\n",
      "Episode: 59\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -19.444751796328283\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.22727790006121\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -2.8715108085832535\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.69567472778294\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 59, Score: 100\n",
      "\n",
      "\n",
      "Episode: 60\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.3744219081056\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.72610725500465\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 60, Score: 100\n",
      "\n",
      "\n",
      "Episode: 61\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -11.809422278979415\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.5098643355405\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.75349652950419\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 61, Score: 100\n",
      "\n",
      "\n",
      "Episode: 62\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.63447405840736\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.77814687655378\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 62, Score: 100\n",
      "\n",
      "\n",
      "Episode: 63\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.74906319334545\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8003321888984\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 63, Score: 100\n",
      "\n",
      "\n",
      "Episode: 64\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.85438976071184\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.82029897000855\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 64, Score: 100\n",
      "\n",
      "\n",
      "Episode: 65\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 96.9511603826715\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8382690730077\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 65, Score: 100\n",
      "\n",
      "\n",
      "Episode: 66\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.04003298263211\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 31.05273721349646\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.12001832259666\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.85444216570693\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 66, Score: 100\n",
      "\n",
      "\n",
      "Episode: 67\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -17.885394802758388\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.19360626474199\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.86899794913623\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 67, Score: 100\n",
      "\n",
      "\n",
      "Episode: 68\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.26127643523228\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.8820981542226\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 68, Score: 100\n",
      "\n",
      "\n",
      "Episode: 69\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.32347650897708\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.89388833880034\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 69, Score: 100\n",
      "\n",
      "\n",
      "Episode: 70\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.38062380362061\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.90449950492031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 70, Score: 100\n",
      "\n",
      "\n",
      "Episode: 71\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.43310687424565\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.91404955442827\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 71, Score: 100\n",
      "\n",
      "\n",
      "Episode: 72\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.4812870927095\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.92264459898544\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 72, Score: 100\n",
      "\n",
      "\n",
      "Episode: 73\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.5255001987381\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.93038013908689\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 73, Score: 100\n",
      "\n",
      "\n",
      "Episode: 74\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.56605781263389\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9373421251782\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 74, Score: 100\n",
      "\n",
      "\n",
      "Episode: 75\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.60324890176314\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 37.510185133421366\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.63672088197947\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.94360791266038\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 75, Score: 100\n",
      "\n",
      "\n",
      "Episode: 76\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.6674659771349\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.94924712139435\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 76, Score: 100\n",
      "\n",
      "\n",
      "Episode: 77\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.69569484443944\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.95432240925491\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 77, Score: 100\n",
      "\n",
      "\n",
      "Episode: 78\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.72160327851174\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.95889016832942\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 78, Score: 100\n",
      "\n",
      "\n",
      "Episode: 79\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.74537307732517\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -6.570915667510994\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.96300115149647\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 79, Score: 100\n",
      "\n",
      "\n",
      "Episode: 80\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -16.420063387827355\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.7671728835908\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.96670103634682\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 80, Score: 100\n",
      "\n",
      "\n",
      "Episode: 81\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.78715899783006\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 43.3400953608644\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.80514650064539\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97003093271213\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 81, Score: 100\n",
      "\n",
      "\n",
      "Episode: 82\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.82166491291936\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 2\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: 48.59043065115698\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.83653148396593\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97302783944092\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 82, Score: 100\n",
      "\n",
      "\n",
      "Episode: 83\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.85020809167399\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97572505549682\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 83, Score: 100\n",
      "\n",
      "\n",
      "Episode: 84\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.86278406300077\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.97815254994714\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 84, Score: 100\n",
      "\n",
      "\n",
      "Episode: 85\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.87434275914546\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98033729495242\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 85, Score: 100\n",
      "\n",
      "\n",
      "Episode: 86\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.8849618754312\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98230356545717\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 86, Score: 100\n",
      "\n",
      "\n",
      "Episode: 87\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.89471374086834\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98407320891145\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 87, Score: 100\n",
      "\n",
      "\n",
      "Episode: 88\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -15.086480388698654\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.90366561446373\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9856658880203\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 88, Score: 100\n",
      "\n",
      "\n",
      "Episode: 89\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.91187997593137\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 0\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: -6.015243177845885\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98709929921827\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 89, Score: 100\n",
      "\n",
      "\n",
      "Episode: 90\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 0\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -10.935203933464269\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.91941480896084\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.98838936929644\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 90, Score: 100\n",
      "\n",
      "\n",
      "Episode: 91\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.9263238756251\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9895504323668\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 91, Score: 100\n",
      "\n",
      "\n",
      "Episode: 92\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.93265698086691\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99059538913012\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 92, Score: 100\n",
      "\n",
      "\n",
      "Episode: 93\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.9384602263041\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.9915358502171\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 93, Score: 100\n",
      "\n",
      "\n",
      "Episode: 94\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.94377625284518\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99238226519539\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 94, Score: 100\n",
      "\n",
      "\n",
      "Episode: 95\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -13.881398500797115\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -12.79682480168573\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 10.790531900000001\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 46.8559\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 95, Score: 100\n",
      "\n",
      "\n",
      "Episode: 96\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 14.25021281\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 52.17031\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 96, Score: 100\n",
      "\n",
      "\n",
      "Episode: 97\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.94864447181502\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99314403867585\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 97, Score: 100\n",
      "\n",
      "\n",
      "Episode: 98\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 2\n",
      "\tNew state: (2, 2)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 12\n",
      "\tQ value: 17.890052219\n",
      "\tState: (2, 2)\n",
      "\n",
      "\tState index: 12\n",
      "\tAction index: 2\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 56.953279\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 98, Score: 100\n",
      "\n",
      "\n",
      "Episode: 99\n",
      "State: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 1\n",
      "\tNew state: (0, 0)\n",
      "\tReward: -100\n",
      "\tIs game over?: False\n",
      "\tNew state index: 0\n",
      "\tQ value: -11.82022651880747\n",
      "\tState: (0, 0)\n",
      "\n",
      "\tState index: 0\n",
      "\tAction index: 3\n",
      "\tNew state: (3, 3)\n",
      "\tReward: -1\n",
      "\tIs game over?: False\n",
      "\tNew state index: 18\n",
      "\tQ value: 97.95310128446242\n",
      "\tState: (3, 3)\n",
      "\n",
      "\tState index: 18\n",
      "\tAction index: 1\n",
      "\tNew state: (4, 4)\n",
      "\tReward: 100\n",
      "\tIs game over?: True\n",
      "\tNew state index: 24\n",
      "\tQ value: 99.99382963480826\n",
      "\tState: (4, 4)\n",
      "\n",
      "\tEpisode: 99, Score: 100\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    logger.info(f'Episode: {episode}')\n",
    "    state: tuple[int, int] = initial_state\n",
    "    logger.info(f'State: {state}\\n')\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    while not is_game_over:\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        logger.info(f'\\tState index: {state_index}')\n",
    "        action_index: int = choose_action(\n",
    "            q_table,\n",
    "            state,\n",
    "            actions,\n",
    "            grid,\n",
    "            epsilon\n",
    "        )\n",
    "        logger.info(f'\\tAction index: {action_index}')\n",
    "        new_state, reward, is_game_over = apply_action(\n",
    "            action_index,\n",
    "            state,\n",
    "            goal_state,\n",
    "            grid,\n",
    "            obstacles\n",
    "        )\n",
    "        logger.info(f'\\tNew state: {new_state}')\n",
    "        logger.info(f'\\tReward: {reward}')\n",
    "        logger.info(f'\\tIs game over?: {is_game_over}')\n",
    "        new_state_index: int = convert_state_to_index(new_state, grid)\n",
    "        logger.info(f'\\tNew state index: {new_state_index}')\n",
    "\n",
    "        q_table[state_index, action_index] = update_Q_values(\n",
    "            q_table,\n",
    "            state_index,\n",
    "            action_index,\n",
    "            alpha,\n",
    "            gamma,\n",
    "            new_state_index,\n",
    "            reward\n",
    "        )\n",
    "        logger.info(f'\\tQ value: {q_table[state_index, action_index]}')\n",
    "\n",
    "        state = new_state\n",
    "        logger.info(f'\\tState: {state}\\n')\n",
    "\n",
    "        if is_game_over:\n",
    "            logger.info(f'\\tEpisode: {episode}, Score: {reward}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizar el proceso de una manera más gráfica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **matriz politica** contiene la **mejor acción** que el agente debe tomar **en cada estado** basado en el conocimiento adquirido durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_matrix: np.ndarray = np.zeros(grid, dtype=int)\n",
    "policy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "### Política aprendida ###\n",
      "\n",
      "[[3 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 2 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "\n",
      "Leyenda:\n",
      "0: arriba\n",
      "1: abajo\n",
      "2: izquierda\n",
      "3: derecha\n"
     ]
    }
   ],
   "source": [
    "# Llenar la matriz con la mejor acción posible para cada estado\n",
    "for i in range(grid[0]):\n",
    "    for j in range(grid[1]):\n",
    "        state: tuple[int, int] = (i, j)\n",
    "        state_index: int = convert_state_to_index(state, grid)\n",
    "        best_action: int = int(np.argmax(q_table[state_index]))\n",
    "        policy_matrix[i, j] = best_action\n",
    "\n",
    "logger.info('### Política aprendida ###\\n')\n",
    "logger.info(policy_matrix)\n",
    "logger.info('\\nLeyenda:')\n",
    "logger.info('0: arriba\\n1: abajo\\n2: izquierda\\n3: derecha')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
