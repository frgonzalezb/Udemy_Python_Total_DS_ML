{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SEG√öN FEDERICO:**\n",
    "\n",
    "Es un tipo de algoritmo de aprendizaje por refuerzo que le permite a un agente imaginario aprender a tomar decisiones √≥ptimas y a alcanzar un objetivo en ese entorno o ambiente determinado.\n",
    "\n",
    "El agente opera **aprendiendo los valores que sean m√°s convenientes para cada paso** que tenga que dar, y a esos pasos los implementamos en un par de datos que definen la acci√≥n que tiene que tomar y el estado en que queda.\n",
    "\n",
    "Entonces, Q-Learning implica un agente que debe identificar qu√© pasos realizar y la **calidad de esos pasos** la vamos a denominar con la letra `Q`.\n",
    "\n",
    "Los valores `Q`, es decir, los valores de sus pasos que se definen por los valores de la acci√≥n y del Estado, van a ayudar a este agente a decidir qu√© acci√≥n tiene que tomar en cada paso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaci√≥n de Bellman para actualizar valores Q\n",
    "\n",
    "![Bellman.png](./assets/Bellman.png)\n",
    "\n",
    "Donde:\n",
    "\n",
    "- `ùëÑ(ùë†,a)` = funci√≥n ùëÑ que se est√° actualizando. Representa el valor actual estimado de tomar la acci√≥n ùëé en el estado ùë†.\n",
    "\n",
    "- `ùë†` = estado (state)\n",
    "\n",
    "- `a` = acci√≥n (action)\n",
    "\n",
    "- `Œ±` = tasa de aprendizaje o **learning rate** (usualmente, un n√∫mero entre 0 y 1). Controla qu√© tan r√°pido actualizamos la estimaci√≥n del valor. Si Œ± es cercano a 1, significa que confiamos mucho en las nuevas experiencias, y si es cercano a 0, confiamos m√°s en los valores previos. Dicho de otra manera, seg√∫n Federico:\n",
    "\n",
    "    - Para un valor cercano a 1, el aprendizaje ser√° m√°s r√°pido, pero menos seguro, con estimaciones menos estables para los valores Q.\n",
    "    \n",
    "    - Para un valor cercano a 0, el aprendizaje ser√° m√°s lento, pero m√°s seguro en llevar una estimaci√≥n m√°s estable de los valores Q.\n",
    "\n",
    "- `R(ùë†,a)` = reforzador (reinforcer) o recompensa (reward), despu√©s de tomar la acci√≥n ùëé en el estado ùë†. Esta recompensa indica el **beneficio obtenido** por realizar esa acci√≥n en ese momento espec√≠fico.\n",
    "\n",
    "- `Œ≥` = factor de descuento o **discount factor**, tambi√©n un n√∫mero entre 0 y 1. Indica cu√°nto valoramos las recompensas futuras. Un valor cercano a 1 significa que nos importan mucho las recompensas futuras (lo suficiente para considerar las consecuencias a largo plazo de nuestras acciones), mientras que un valor cercano a 0 significa que solo nos interesa la recompensa inmediata.\n",
    "\n",
    "- `ùë†‚Ä≤` = nuevo estado al que llega el agente despu√©s de tomar la acci√≥n ùëé en el estado ùë†.\n",
    "\n",
    "- `max a‚Ä≤ Q(ùë†‚Ä≤, a‚Ä≤)` = selecciona el valor m√°ximo de la funci√≥n ùëÑ sobre todas las acciones posibles ùëé‚Ä≤ que el agente podr√≠a tomar en el nuevo estado ùë†‚Ä≤. En otras palabras, el agente trata de estimar el mejor valor futuro posible en el siguiente estado, bas√°ndose en lo que ha aprendido hasta el momento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de ejemplo\n",
    "\n",
    "Nos imaginemos un robot que opera en una cuadr√≠cula que representa el sal√≥n de una f√°brica en la que debe moverse trasladando herramientas.\n",
    "\n",
    "El objetivo es que el robot aprenda a encontrar el camino m√°s corto, desde su posici√≥n inicial hasta el destino, evitando obst√°culos.\n",
    "\n",
    "Entonces, el entorno que vamos a definir para este ejercicio va a ser una cuadr√≠cula de dimensiones de 5x5, un punto de inicio en la esquina superior izquierda, que ser√≠a el punto `(0, 0)`, el punto objetivo en la esquina inferior derecha, que ser√≠a el punto `(4, 4)` y los obst√°culos distribuidos en la cuadr√≠cula.\n",
    "\n",
    "Vamos a establecer las acciones. Esto implica que el robot va a poder moverse en cuatro direcciones hacia arriba, hacia abajo, izquierda o derecha, y vamos a definir recompensas.\n",
    "\n",
    "Si alcanza el objetivo, va a tener 100 puntos de premio.\n",
    "\n",
    "Si colisiona con un obst√°culo, se le van a restar 100 puntos y cualquier otro movimiento que haga va a valer -1.\n",
    "\n",
    "¬øPara que?\n",
    "\n",
    "Para incentivar la eficiencia, que reste lo menos que pueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid: tuple[int, int] = (5, 5)\n",
    "\n",
    "initial_state: tuple[int, int] = (0, 0)\n",
    "goal_state: tuple[int, int] = (4, 4)\n",
    "\n",
    "obstacles: list[tuple[int, int]] = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "\n",
    "actions: dict[str, tuple[int, int]] = {\n",
    "    'up': (0, -1),\n",
    "    'down': (0, 1),\n",
    "    'left': (-1, 0),\n",
    "    'right': (1, 0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_states: int = grid[0] * grid[1]\n",
    "grid_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_actions: int = len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table: np.ndarray = np.zeros((grid_states, possible_actions))\n",
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = convert_state_to_index(initial_state, grid)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¬øC√≥mo el agente robot va a aprender a navegar por el entorno?**\n",
    "\n",
    "Los siguientes par√°metros son fundamentales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha: float = 0.1      # learning rate\n",
    "gamma: float = 0.99     # discount factor\n",
    "epsilon: float = 0.2    # exploration\n",
    "episodes: int = 100     # number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `epsilon` sirve para que el agente no repita siempre las mismas acciones, definiendo la **probabilidad** de que el agente tome una **acci√≥n aleatoria** en lugar de la mejor acci√≥n conocida hasta el momento, seg√∫n la tabla Q.\n",
    "\n",
    "En otras palabras, esto permite que el agente **explore el entorno** desconocido en lugar de explotar constantemente el conocimiento que ya dispone, porque una vez que encuentre una soluci√≥n, podr√≠a tender a repetir esa soluci√≥n siempre, en vez de explorar otras posibilidades (que podr√≠an ser m√°s eficientes o de mayor inter√©s, seg√∫n sea el caso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `episodes` est√° definiendo el n√∫mero total de veces (episodios) que se va a repetir este proceso de entrenamiento.\n",
    "\n",
    "Un episodio comienza con el agente en el estado inicial (declarado en `initial_state`) y termina cuando alcanza el estado objetivo (declarado en `goal_state`) o alg√∫n otro criterio de finalizaci√≥n, seg√∫n sea requerido.\n",
    "\n",
    "Mientras mayor sea el n√∫mero de episodios, m√°s oportunidades va a tener el agente de tener m√°s experiencias de las cuales aprender y esto va a mejorar potencialmente sus pol√≠ticas de acci√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
