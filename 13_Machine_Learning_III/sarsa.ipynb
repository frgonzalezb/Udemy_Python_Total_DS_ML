{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA en Machine Learning - Parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El acrónimo SARSA proviene de las siglas en inglés de la expresión: _estado, acción recompensa, estado, acción_. Es decir: _State, Action, Reward, State, Action_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA es un algoritmo que se utiliza para **aprender la política de un agente**, entendiendo como **mejor política** la _mejor acción que se pueda tomar en cada estado o situación_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARSA es bastante similar a Q-Learning, pero con la diferencia de que mientras Q-Learning solamente considera el estado actual del agente (en qué casilla estaba el robot, por recordar el ejemplo visto en Q-Learning), **SARSA toma en cuenta tanto el estado actual como la acción actual para predecir la próxima acción y su recompensa**.\n",
    "\n",
    "Este enfoque se conoce como **aprendizaje on policy**, donde el agente **aprende el valor de la política** que está siendo seguida actualmente, incluyendo cómo va a decidir sus acciones futuras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La actualización de los valores Q en SARSA sigue prácticamente la misma fórmula vista en Q-Learning (ecuación de Bellman), con la diferencia de que la recompensa observada se indica por una `r` (minúscula).\n",
    "\n",
    "![SARSA.png](./assets/SARSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "import logging as lg\n",
    "import random as rnd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid: tuple[int, int] = (4, 4)\n",
    "\n",
    "initial_state: tuple[int, int] = (0, 0)\n",
    "goal_state: tuple[int, int] = (3, 3)\n",
    "\n",
    "actions: list[tuple[int, int]] = [\n",
    "    (0, -1),    # up\n",
    "    (0, 1),     # down\n",
    "    (-1, 0),    # left\n",
    "    (1, 0),     # right\n",
    "]\n",
    "\n",
    "action_symbols: list[str] = ['↑', '↓', '→', '←']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_possible_states: int = grid[0] * grid[1]\n",
    "max_possible_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_possible_actions: int = len(actions)\n",
    "max_possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((max_possible_states, max_possible_actions))\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA parameters\n",
    "alpha: float = 0.1\n",
    "gamma: float = 0.99\n",
    "epsilon: float = 0.2\n",
    "episodes: int = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_index(\n",
    "        state: tuple[int, int],\n",
    "        grid: tuple[int, int]\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Convert the current two-dimensional representation of the current\n",
    "    state (the agent's position on the grid) to an unique linear index.\n",
    "\n",
    "    Every possible state can be represented as a index number for the\n",
    "    Q table.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The index of the current state in the Q table.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> convert_state_to_index((0, 0), (5, 5))\n",
    "    0\n",
    "    >>> convert_state_to_index((4, 4), (5, 5))\n",
    "    24\n",
    "\n",
    "    \"\"\"\n",
    "    return state[0] * grid[1] + state[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example: int = convert_state_to_index((3, 0), grid)\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entender mejor el resultado de `example`:\n",
    "\n",
    "![./assets/ejemplo_cuadricula_4x4.png](./assets/4x4_grid_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(\n",
    "        Q: np.ndarray,\n",
    "        state: tuple[int, int],\n",
    "        max_actions: int,\n",
    "        grid: tuple[int, int],\n",
    "        epsilon: float\n",
    "        ) -> int:\n",
    "    \"\"\"\n",
    "    Choose a random action or the best action for the current state,\n",
    "    depending on the exploration rate.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : np.ndarray\n",
    "        The Q table.\n",
    "    \n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "    \n",
    "    max_actions : int\n",
    "        The maximum possible actions for the agent. \n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    epsilon : float\n",
    "        The exploration rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        A random action or the best action for the current state,\n",
    "        depending on the exploration rate.\n",
    "    \n",
    "    Requirements\n",
    "    ------------\n",
    "    1. Python's built-in `random` library (`import random as rnd`).\n",
    "    2. NumPy (`import numpy as np`).\n",
    "    3. The `convert_state_to_index()` function declared above.\n",
    "    \"\"\"\n",
    "    random_number: float = rnd.uniform(0, 1)\n",
    "    random_action: int = rnd.randint(0, max_actions - 1)\n",
    "    best_action: int = int(np.argmax(Q[convert_state_to_index(state, grid)]))\n",
    "\n",
    "    if random_number < epsilon:\n",
    "        return random_action\n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_action(\n",
    "        action: int,\n",
    "        state: tuple[int, int],\n",
    "        goal_state: tuple[int, int],\n",
    "        grid: tuple[int, int],\n",
    "        obstacles: list[tuple[int, int]]\n",
    "        ) -> tuple[tuple[int, int], int, bool]:\n",
    "    \"\"\"\n",
    "    Apply the action to the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    action : int\n",
    "        The action to be applied.\n",
    "\n",
    "    state : tuple[int, int]\n",
    "        The current two-dimensional representation of the current state\n",
    "        (the agent's position on the grid).\n",
    "\n",
    "    goal_state : tuple[int, int]\n",
    "        The two-dimensional representation of the state declared as\n",
    "        goal.\n",
    "\n",
    "    grid : tuple[int, int]\n",
    "        The width and height of the grid.\n",
    "\n",
    "    obstacles : list[tuple[int, int]]\n",
    "        The list of obstacles on the grid.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[tuple[int, int], int, bool]\n",
    "        The new state, the reward, and whether the game is over.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    - NumPy (`import numpy as np`).\n",
    "    \"\"\"\n",
    "    reward: int = 0\n",
    "    new_state: tuple[int, int] = tuple(np.add(state, action) % np.array(grid))\n",
    "    is_game_over: bool = False\n",
    "\n",
    "    if new_state == goal_state:\n",
    "        reward = 1\n",
    "        is_game_over: bool = True\n",
    "    else:\n",
    "        reward = -1\n",
    "    \n",
    "    return new_state, reward, is_game_over\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
