{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte I (1)\n",
    "Implementa la función `indice_a_estado`, que toma un índice lineal (número entero) y lo convierte de nuevo a una representación bidimensional (tupla) del estado actual en la cuadrícula. Esta función es la inversa de `estado_a_indice` que vimos en la lección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "def indice_a_estado(\n",
    "        indice_lineal: int,\n",
    "        ) -> tuple[int, int]:\n",
    "    estado_0, estado_1 = divmod(indice_lineal, 5)\n",
    "    return estado_0, estado_1\n",
    "\n",
    "\n",
    "print(indice_a_estado(24))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTA PERSONAL:** Esta es la solución aceptada en Udemy, pero no me parece que el valor `5` en `divmod()` deba ser hardcoded. Lamentablemente, el ejercicio no pemite pasar un segundo argumento a la función. 😞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning en Machine Learning - Parte I (2)\n",
    "Escribe una función `crear_entorno()` que inicialice el entorno con las condiciones dadas: **dimensiones de la cuadrícula, estado_inicial, estado_objetivo y obstáculos.**\n",
    "\n",
    "1. Define una cuadrícula con dimensiones de 5x5. `dimensiones`\n",
    "\n",
    "2. El punto de inicio está en la esquina superior izquierda (0,0). `estado_inicial`\n",
    "\n",
    "3. El punto objetivo está en la esquina inferior derecha (4,4). `estado_objetivo`\n",
    "\n",
    "4. Obstáculos distribuidos en: (1,1), (1,3), (2,3), (3,0). `obstaculos`\n",
    "\n",
    "Asegurate que esta función devuelva todos los valores requeridos:\n",
    "\n",
    "`dimensiones`, `estado_inicial`, `estado_objetivo`, `obstaculos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypeAlias\n",
    "\n",
    "\n",
    "EnvironmentType: TypeAlias = tuple[\n",
    "    tuple[int, int],\n",
    "    tuple[int, int],\n",
    "    tuple[int, int],\n",
    "    list[tuple[int, int]]\n",
    "]\n",
    "\n",
    "\n",
    "def crear_entorno() -> EnvironmentType:\n",
    "    dimensiones: tuple[int, int] = (5, 5)\n",
    "    estado_inicial: tuple[int, int] = (0, 0)\n",
    "    estado_objetivo: tuple[int, int] = (4, 4)\n",
    "    obstaculos: list[tuple[int, int]] = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "    return dimensiones, estado_inicial, estado_objetivo, obstaculos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning en Machine Learning - Parte II (1)\n",
    "\n",
    "En este ejercicio, desarrollarás una función denominada `elegir_accion`, la cual selecciona una acción basada en la estrategia epsilon-greedy. Esta estrategia es fundamental en el aprendizaje por refuerzo para equilibrar entre la exploración (seleccionar acciones al azar) y la explotación (seleccionar la mejor acción conocida).\n",
    "\n",
    "**Datos Iniciales:**\n",
    "\n",
    "Se te proporcionan los siguientes componentes iniciales en Python:\n",
    "\n",
    "- `epsilon`: Un valor flotante que representa la probabilidad de tomar una acción aleatoria. El valor es 0.1.\n",
    "\n",
    "- `num_acciones`: Un entero que indica el número total de acciones posibles, que en este caso es 4.\n",
    "\n",
    "- `Q`: Una matriz de ceros con dimensiones 25x4 que representa los valores estimados de cada acción en cada estado.\n",
    "\n",
    "- `dimensiones`: Una tupla que indica las dimensiones de un espacio de estados estructurado en forma de cuadrícula (5x5).\n",
    "\n",
    "**Funciones Proporcionadas:**\n",
    "\n",
    "- `estado_a_indice(estado)`: Esta función convierte un estado, representado como una tupla de dos elementos (fila, columna), en un índice lineal para acceder a la matriz `Q`.\n",
    "\n",
    "### Instrucciones:\n",
    "\n",
    "1. Completa la función `elegir_accion`:\n",
    "\n",
    "    - **Parámetro de entrada:** `estado`, una tupla que representa el estado actual en forma de (fila, columna).\n",
    "\n",
    "    - **Valor de retorno:** Debe devolver un entero que representa el índice de la acción elegida.\n",
    "\n",
    "    - **Comportamiento esperado:**\n",
    "\n",
    "        - Con una probabilidad `epsilon`, la función debe elegir y retornar una acción de manera aleatoria.\n",
    "\n",
    "        - Con una probabilidad `1 - epsilon`, la función debe retornar la acción que tiene el mayor valor en la matriz `Q` para el estado dado.\n",
    "\n",
    "2. Utiliza la función `random.uniform(0, 1)` para generar un número aleatorio y compararlo con `epsilon` para decidir si se explora o se explota.\n",
    "\n",
    "3. Utiliza `random.choice(range(num_acciones))` para seleccionar una acción aleatoria durante la fase de exploración.\n",
    "\n",
    "4. Utiliza `np.argmax()` para seleccionar la acción con el mayor valor de `Q` durante la fase de explotación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Parámetros globales para la función elegir_accion\n",
    "epsilon: float = 0.1\n",
    "num_acciones: int = 4\n",
    "Q: np.ndarray = np.zeros((25, num_acciones))  # Ejemplo de matriz Q\n",
    "dimensiones: tuple[int, int] = (5, 5)\n",
    "\n",
    "def estado_a_indice(estado: tuple[int, int]) -> int:\n",
    "    # Convierte el estado en un índice para la matriz Q\n",
    "    return estado[0] * dimensiones[1] + estado[1]\n",
    "\n",
    "def elegir_accion(estado: tuple[int, int]) -> int:\n",
    "    # Implementa la estrategia epsilon-greedy\n",
    "    # Exploración: elige una acción aleatoria\n",
    "    # Explotación: elige la mejor acción según la matriz Q\n",
    "    numero_aleatorio: float = random.uniform(0, 1)\n",
    "    accion_aleatoria: int = random.choice(range(num_acciones))\n",
    "    mejor_accion: int = int(np.argmax(Q[estado_a_indice(estado)]))\n",
    "\n",
    "    if numero_aleatorio < epsilon:\n",
    "        return accion_aleatoria\n",
    "    return mejor_accion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q - Learning en Machine Learning - Parte II (2)\n",
    "Implementa una función llamada `aplicar_accion`. Esta función será parte de un sistema de navegación en un grid donde un agente puede moverse y encontrar un objetivo mientras evita obstáculos.\n",
    "\n",
    "**Variables de Configuración:**\n",
    "\n",
    "- `acciones`: Una lista de tuplas que representan las direcciones de movimiento posibles. Estas direcciones son: izquierda, abajo, derecha y arriba.\n",
    "\n",
    "- `dimensiones`: Una tupla que define las dimensiones del grid (5x5).\n",
    "\n",
    "- `obstaculos`: Una lista de tuplas que indica las posiciones de los obstáculos en el grid.\n",
    "\n",
    "- `estado_objetivo`: Una tupla que señala la posición del objetivo en el grid.\n",
    "\n",
    "```\n",
    "acciones = [(0, -1), (1, 0), (0, 1), (-1, 0)]\n",
    "dimensiones = (5, 5)\n",
    "obstaculos = [(2, 2), (3, 3)]\n",
    "estado_objetivo = (4, 4)\n",
    "```\n",
    "\n",
    "**Objetivo:**\n",
    "\n",
    "Tu tarea es implementar la función `aplicar_accion` utilizando las variables de configuración proporcionadas. Debes asegurarte de que la función maneje correctamente los movimientos hacia los obstáculos, el movimiento fuera de los límites del grid, y la identificación cuando el agente alcanza el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Variables de configuración\n",
    "acciones = [(0, -1), (1, 0), (0, 1), (-1, 0)]  # Movimientos: izquierda, abajo, derecha, arriba\n",
    "dimensiones = (5, 5)  # Dimensiones del grid\n",
    "obstaculos = [(2, 2), (3, 3)]  # Posiciones de obstáculos\n",
    "estado_objetivo = (4, 4)  # Posición del objetivo\n",
    "\n",
    "def aplicar_accion(estado, accion_idx):\n",
    "    \"\"\"\n",
    "    Aplica una acción basada en un índice de acción dado y actualiza el estado del agente en el grid.\n",
    "\n",
    "    Parámetros:\n",
    "    - estado (tuple): La posición actual del agente en el grid.\n",
    "    - accion_idx (int): El índice de la acción a aplicar, que está basado en la lista 'acciones'.\n",
    "\n",
    "    Retorna:\n",
    "    - tuple: El nuevo estado del agente después de aplicar la acción.\n",
    "    - int: La recompensa o penalización resultante de la acción.\n",
    "    - bool: Un booleano que indica si el objetivo ha sido alcanzado.\n",
    "    \"\"\"\n",
    "    puntos = 0\n",
    "    terminado = False\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = (estado[0] + accion[0], estado[1] + accion[1])\n",
    "\n",
    "    if (0 <= nuevo_estado[0] < dimensiones[0]) and (0 <= nuevo_estado[1] < dimensiones[1]):\n",
    "        if nuevo_estado in obstaculos:\n",
    "            puntos = -100\n",
    "            return estado, puntos, terminado\n",
    "        elif nuevo_estado == estado_objetivo:\n",
    "            puntos = 100\n",
    "            terminado = True\n",
    "            return nuevo_estado, puntos, terminado\n",
    "        else:\n",
    "            puntos = -1\n",
    "            return nuevo_estado, puntos, terminado\n",
    "    else:\n",
    "        puntos = -1\n",
    "        return nuevo_estado, puntos, terminado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
